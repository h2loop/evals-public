[
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "13_geometry_closest_pair_2d",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of points. The final result is returned on rank 0.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(std::vector<Point> const& points) {",
        "outputs": [
            {
                "generated_output": " \n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (points.empty()) {\n        return 0.0;\n    }\n\n    double minDistance = std::numeric_limits<double>::max();\n\n    for (int i = rank; i < points.size(); i += size) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n\n    // Reduce the minimum distances to rank 0\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalMinDistance;\n    }\n    return 0.0;\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": true,
                "are_all_valid": true,
                "best_sequential_runtime": 0.0,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.3287558,
                        "num_procs": 1
                    },
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.1486442,
                        "num_procs": 2
                    },
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.087763,
                        "num_procs": 4
                    }
                ]
            }
        ],
        "response_time": 4.135987758636475,
        "total_tokens": 1644,
        "prompt_tokens": 1389
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "13_geometry_closest_pair_2d",
        "parallelism_model": "serial",
        "prompt": "struct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(std::vector<Point> const& points) {",
        "outputs": [
            {
                "generated_output": " \nif (points.size() < 2) {\n    return 0.0;\n}\n\ndouble minDistance = std::numeric_limits<double>::infinity();\n\n#pragma omp parallel for reduction(min:minDistance)\nfor (size_t i = 0; i < points.size(); ++i) {\n    for (size_t j = i + 1; j < points.size(); ++j) {\n        double dist = distance(points[i], points[j]);\n        if (dist < minDistance) {\n            minDistance = dist;\n        }\n    }\n}\n\nreturn minDistance;\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": true,
                "are_all_valid": true,
                "best_sequential_runtime": 4.16e-08,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 9.16e-08
                    }
                ]
            }
        ],
        "response_time": 2.7939438819885254,
        "total_tokens": 1495,
        "prompt_tokens": 1343
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "11_geometry_convex_hull_perimeter",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the perimeter of the smallest convex polygon that contains all the points in the vector points.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of points. The final result is returned on rank 0.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: 13.4477\n*/\ndouble convexHullPerimeter(std::vector<Point> const& points) {",
        "outputs": [
            {
                "generated_output": " \n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int numPoints = points.size();\n    std::vector<Point> localPoints(numPoints / size);\n\n    MPI_Scatter(points.data(), numPoints / size, MPI_DOUBLE_INT, localPoints.data(), numPoints / size, MPI_DOUBLE_INT, 0, MPI_COMM_WORLD);\n\n    double localPerimeter = 0.0;\n    for (int i = 0; i < localPoints.size(); ++i) {\n        for (int j = i + 1; j < localPoints.size(); ++j) {\n            double dist = distance(localPoints[i], localPoints[j]);\n            localPerimeter += dist;\n        }\n    }\n\n    double globalPerimeter = 0.0;\n    MPI_Reduce(&localPerimeter, &globalPerimeter, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalPerimeter;\n    }\n    return 0.0;\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 1
                    },
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 2
                    },
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 4
                    }
                ]
            }
        ],
        "response_time": 4.073138952255249,
        "total_tokens": 1694,
        "prompt_tokens": 1402
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "11_geometry_convex_hull_perimeter",
        "parallelism_model": "serial",
        "prompt": "struct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the perimeter of the smallest convex polygon that contains all the points in the vector points.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: 13.4477\n*/\ndouble convexHullPerimeter(std::vector<Point> const& points) {",
        "outputs": [
            {
                "generated_output": " \n#include <vector>\n#include <algorithm>\n#include <cmath>\n\nstruct Point {\n    double x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n    return std::sqrt(std::pow(p2.x - p1.x, 2) + std::pow(p2.y - p1.y, 2));\n}\n\n// Helper function to sort points by polar angle with respect to the pivot\nbool compare(Point p1, Point p2, Point pivot) {\n    double crossProduct = (p1.x - pivot.x) * (p2.y - pivot.y) - (p2.x - pivot.x) * (p1.y - pivot.y);\n    if (crossProduct == 0) {\n        return distance(pivot, p1) < distance(pivot, p2);\n    }\n    return crossProduct > 0;\n}\n\nstd::vector<Point> convexHull(std::vector<Point> points) {\n    if (points.size() <= 1) return points;\n\n    // Find the point with the lowest y-coordinate (and leftmost in case of tie)\n    Point pivot = points[0];\n    int n = points.size();\n    for (int i = 1; i < n; ++i) {\n        if (points[i].y < pivot.y || (points[i].y == pivot.y && points[i].x < pivot.x)) {\n            pivot = points[i];\n        }\n    }\n\n    // Sort the points based on the polar angle with respect to the pivot\n    std::sort(points.begin(), points.end(), [pivot](Point a, Point b) {\n        return compare(a, b, pivot);\n    });\n\n    // Create the convex hull\n    std::vector<Point> hull;\n    hull.push_back(points[0]);\n    hull.push_back(points[1]);\n\n    for (int i = 2; i < n; ++i) {\n        while (hull.size() > 1 && ((hull.back().x - hull[hull.size() - 2].x) * (points[i].y - hull[hull.size() - 2].y) -\n                                   (hull.back().y - hull[hull.size() - 2].y) * (points[i].x - hull[hull.size() - 2].x)) <= 0) {\n            hull.pop_back();\n        }\n        hull.push_back(points[i]);\n    }\n\n    return hull;\n}\n\ndouble convexHullPerimeter(std::vector<Point> const& points) {\n    if (points.empty()) return 0.0;\n\n    std::vector<Point> hull = convexHull(points);\n    double perimeter = 0.0;\n    int n = hull.size();\n    for (int i = 0; i < n; ++i) {\n        perimeter += distance(hull[i], hull[(i + 1) % n]);\n    }\n\n    return perimeter;\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 17.59954023361206,
        "total_tokens": 2099,
        "prompt_tokens": 1356
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "12_geometry_smallest_triangle",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble triangleArea(Point const& A, Point const& B, Point const& C) {\n  return 0.5 * std::abs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Example:\n\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(std::vector<Point> const& points) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nif (points.size() < 3) {\n    if (rank == 0) {\n        return 0.0;\n    }\n    return MPI_DOUBLE_MAX;\n}\n\ndouble localMinArea = std::numeric_limits<double>::max();\n\nfor (size_t i = rank; i < points.size(); i += size) {\n    for (size_t j = i + 1; j < points.size(); j++) {\n        for (size_t k = j + 1; k < points.size(); k++) {\n            double area = triangleArea(points[i], points[j], points[k]);\n            if (area < localMinArea) {\n                localMinArea = area;\n            }\n        }\n    }\n}\n\ndouble globalMinArea;\nMPI_Reduce(&localMinArea, &globalMinArea, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\nif (rank == 0) {\n    return globalMinArea;\n}\nreturn MPI_DOUBLE_MAX;\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 8.636667251586914,
        "total_tokens": 1664,
        "prompt_tokens": 1370
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "12_geometry_smallest_triangle",
        "parallelism_model": "serial",
        "prompt": "struct Point {\n\tdouble x, y;\n};\n\ndouble triangleArea(Point const& A, Point const& B, Point const& C) {\n  return 0.5 * std::abs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Example:\n\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(std::vector<Point> const& points) {",
        "outputs": [
            {
                "generated_output": " \nif (points.size() < 3) {\n    return 0.0;\n}\n\ndouble minArea = std::numeric_limits<double>::max();\n\n#pragma omp parallel for reduction(min:minArea)\nfor (size_t i = 0; i < points.size(); ++i) {\n    for (size_t j = i + 1; j < points.size(); ++j) {\n        for (size_t k = j + 1; k < points.size(); ++k) {\n            double area = triangleArea(points[i], points[j], points[k]);\n            if (area < minArea) {\n                minArea = area;\n            }\n        }\n    }\n}\n\nreturn minArea;\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": true,
                "are_all_valid": true,
                "best_sequential_runtime": 2.52e-08,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 3.34e-08
                    }
                ]
            }
        ],
        "response_time": 12.52774691581726,
        "total_tokens": 1528,
        "prompt_tokens": 1345
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "14_geometry_closest_pair_1d",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\ndouble distance(double x1, double x2) {\n\treturn std::abs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Example: \n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {",
        "outputs": [
            {
                "generated_output": " \n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (x.empty()) {\n        return 0.0;\n    }\n\n    // Each process handles a subset of the data\n    int local_size = x.size() / size;\n    int remainder = x.size() % size;\n    int start = rank * local_size + std::min(rank, remainder);\n    int end = start + local_size + (rank < remainder ? 1 : 0);\n\n    double min_distance = std::numeric_limits<double>::max();\n\n    for (int i = start; i < end; ++i) {\n        for (int j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < min_distance) {\n                min_distance = dist;\n            }\n        }\n    }\n\n    // Reduce the minimum distances to find the global minimum\n    double global_min_distance;\n    MPI_Reduce(&min_distance, &global_min_distance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_min_distance;\n    }\n    return 0.0;\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": true,
                "are_all_valid": true,
                "best_sequential_runtime": 0.0,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.0601796,
                        "num_procs": 1
                    },
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.0359354,
                        "num_procs": 2
                    },
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.0230701,
                        "num_procs": 4
                    }
                ]
            }
        ],
        "response_time": 6.2194530963897705,
        "total_tokens": 1635,
        "prompt_tokens": 1293
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "14_geometry_closest_pair_1d",
        "parallelism_model": "serial",
        "prompt": "double distance(double x1, double x2) {\n\treturn std::abs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Example: \n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {",
        "outputs": [
            {
                "generated_output": " \nif (x.size() < 2) {\n    throw std::invalid_argument(\"Vector must contain at least two elements.\");\n}\n\ndouble minDistance = std::numeric_limits<double>::max();\n\n#pragma omp parallel for reduction(min:minDistance)\nfor (size_t i = 0; i < x.size(); ++i) {\n    for (size_t j = i + 1; j < x.size(); ++j) {\n        double dist = distance(x[i], x[j]);\n        if (dist < minDistance) {\n            minDistance = dist;\n        }\n    }\n}\n\nreturn minDistance;\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": true,
                "are_all_valid": true,
                "best_sequential_runtime": 3.34e-08,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.2363389418
                    }
                ]
            }
        ],
        "response_time": 3.1345131397247314,
        "total_tokens": 1429,
        "prompt_tokens": 1269
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "10_geometry_convex_hull",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Find the set of points that defined the smallest convex polygon that contains all the points in the vector points. Store the result in `hull`.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of points. The final hull is returned on rank 0.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: [{0, 3}, {4, 4}, {3, 1}, {0, 0}]\n*/\nvoid convexHull(std::vector<Point> const& points, std::vector<Point> &hull) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n// Distribute points among ranks\nstd::vector<Point> localPoints;\nint pointsPerRank = points.size() / size;\nint remainder = points.size() % size;\nint start = rank * pointsPerRank;\nint end = start + pointsPerRank;\nif (rank == size - 1) {\n    end += remainder;\n}\nfor (int i = start; i < end; ++i) {\n    localPoints.push_back(points[i]);\n}\n\n// Perform convex hull computation on each rank\nstd::vector<Point> localHull = computeConvexHull(localPoints);\n\n// Gather the local hulls on rank 0\nMPI_Barrier(MPI_COMM_WORLD);\nif (rank == 0) {\n    std::vector<std::vector<Point>> allHulls(size);\n    MPI_Gather(&localHull[0], localHull.size(), pointType, &allHulls[0], localHull.size(), pointType, 0, MPI_COMM_WORLD);\n\n    // Merge all hulls on rank 0\n    if (!allHulls.empty()) {\n        hull = allHulls[0];\n        for (int i = 1; i < size; ++i) {\n            hull = mergeHulls(hull, allHulls[i]);\n        }\n    }\n} else {\n    MPI_Gather(&localHull[0], localHull.size(), pointType, nullptr, 0, MPI_COMM_WORLD);\n}\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 5.247241973876953,
        "total_tokens": 1776,
        "prompt_tokens": 1376
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "10_geometry_convex_hull",
        "parallelism_model": "serial",
        "prompt": "struct Point {\n   double x, y;\n};\n\n/* Find the set of points that defined the smallest convex polygon that contains all the points in the vector points. Store the result in `hull`.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: [{0, 3}, {4, 4}, {3, 1}, {0, 0}]\n*/\nvoid convexHull(std::vector<Point> const& points, std::vector<Point> &hull) {",
        "outputs": [
            {
                "generated_output": " \n#include <vector>\n#include <algorithm>\n#include <cmath>\n\nstruct Point {\n    double x, y;\n};\n\nbool comparePoints(const Point& a, const Point& b) {\n    return a.x < b.x || (a.x == b.x && a.y < b.y);\n}\n\ndouble crossProduct(const Point& O, const Point& A, const Point& B) {\n    return (A.x - O.x) * (B.y - O.y) - (A.y - O.y) * (B.x - O.x);\n}\n\nvoid convexHull(std::vector<Point> const& points, std::vector<Point> &hull) {\n    if (points.size() <= 1) {\n        hull = points;\n        return;\n    }\n\n    std::vector<Point> sortedPoints = points;\n    std::sort(sortedPoints.begin(), sortedPoints.end(), comparePoints);\n\n    // Build lower hull \n    for (size_t i = 0; i < sortedPoints.size(); ++i) {\n        while (hull.size() >= 2 && crossProduct(hull[hull.size() - 2], hull[hull.size() - 1], sortedPoints[i]) <= 0) {\n            hull.pop_back();\n        }\n        hull.push_back(sortedPoints[i]);\n    }\n\n    // Build upper hull\n    size_t t = hull.size() + 1;\n    for (size_t i = sortedPoints.size() - 1; i > 0; --i) {\n        while (hull.size() >= t && crossProduct(hull[hull.size() - 2], hull[hull.size() - 1], sortedPoints[i - 1]) <= 0) {\n            hull.pop_back();\n        }\n        hull.push_back(sortedPoints[i - 1]);\n    }\n\n    hull.pop_back();\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 6.412745952606201,
        "total_tokens": 1817,
        "prompt_tokens": 1331
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "59_transform_map_function",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nbool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored in mask on rank 0.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {",
        "outputs": [
            {
                "generated_output": " \n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<bool> localMask(n / size + (rank == size - 1 ? n % size : 0));\n\n    for (int i = rank; i < n; i += size) {\n        localMask[i / size] = isPowerOfTwo(x[i]);\n    }\n\n    std::vector<bool> globalMask(n);\n    MPI_Allgather(localMask.data(), localMask.size(), MPI_C_BOOL, globalMask.data(), localMask.size(), MPI_C_BOOL, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        mask = globalMask;\n    }\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 8.21578311920166,
        "total_tokens": 1566,
        "prompt_tokens": 1354
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "59_transform_map_function",
        "parallelism_model": "serial",
        "prompt": "bool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {",
        "outputs": [
            {
                "generated_output": " \n    mask.resize(x.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": true,
                "are_all_valid": true,
                "best_sequential_runtime": 0.0062875916,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.006075425
                    }
                ]
            }
        ],
        "response_time": 2.7616848945617676,
        "total_tokens": 1372,
        "prompt_tokens": 1306
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "57_transform_inverse_offset",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Replace every element of the vector x with 1-1/x.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(std::vector<double> &x) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nstd::vector<double> localResults(x.size() / size + (rank < x.size() % size ? 1 : 0));\n\nfor (size_t i = rank; i < x.size(); i += size) {\n    localResults[i / size] = (x[i] != 0) ? 1 - 1 / x[i] : 0;\n}\n\nMPI_Allgather(localResults.data(), localResults.size(), MPI_DOUBLE, x.data(), localResults.size(), MPI_DOUBLE, MPI_COMM_WORLD);\n\nif (rank == 0) {\n    for (auto &val : x) {\n        val = (val != 0) ? 1 - 1 / val : 0;\n    }\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 3.9940600395202637,
        "total_tokens": 1539,
        "prompt_tokens": 1313
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "57_transform_inverse_offset",
        "parallelism_model": "serial",
        "prompt": "/* Replace every element of the vector x with 1-1/x.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(std::vector<double> &x) {",
        "outputs": [
            {
                "generated_output": " \nfor (auto &value : x) {\n    if (value != 0) {\n        value = 1 - 1 / value;\n    } else {\n        value = 0; // Handle division by zero\n    }\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 4.716785907745361,
        "total_tokens": 1326,
        "prompt_tokens": 1267
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "55_transform_relu",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(std::vector<double> &x) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nstd::vector<double> localX(x.size() / size);\nMPI_Scatter(x.data(), localX.size(), MPI_DOUBLE, localX.data(), localX.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\nfor (auto &value : localX) {\n    if (value < 0) {\n        value = 0;\n    }\n}\n\nMPI_Gather(localX.data(), localX.size(), MPI_DOUBLE, x.data(), localX.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 4.9108521938323975,
        "total_tokens": 1525,
        "prompt_tokens": 1342
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "55_transform_relu",
        "parallelism_model": "serial",
        "prompt": "/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(std::vector<double> &x) {",
        "outputs": [
            {
                "generated_output": " \nfor (auto& value : x) {\n    if (value < 0) {\n        value = 0;\n    }\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 4.564824104309082,
        "total_tokens": 1333,
        "prompt_tokens": 1296
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "56_transform_negate_odds",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(std::vector<int> &x) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nfor (int i = rank; i < x.size(); i += size) {\n    if (x[i] % 2 == 0) {\n        x[i] /= 2;\n    } else {\n        x[i] = -x[i];\n    }\n}\n\nif (rank == 0) {\n    MPI_Reduce(MPI_IN_PLACE, x.data(), x.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n} else {\n    MPI_Reduce(x.data(), nullptr, x.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 3.44541597366333,
        "total_tokens": 1523,
        "prompt_tokens": 1322
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "56_transform_negate_odds",
        "parallelism_model": "serial",
        "prompt": "/* In the vector x negate the odd values and divide the even values by 2.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(std::vector<int> &x) {",
        "outputs": [
            {
                "generated_output": " \nfor (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n        x[i] /= 2;\n    } else {\n        x[i] = -x[i];\n    }\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 2.915390968322754,
        "total_tokens": 1350,
        "prompt_tokens": 1276
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "58_transform_squaring",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Replace every element of x with the square of its value.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nfor (size_t i = rank; i < x.size(); i += size) {\n    x[i] = x[i] * x[i];\n}\n\nif (rank == 0) {\n    MPI_Reduce(MPI_IN_PLACE, x.data(), x.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n} else {\n    MPI_Reduce(x.data(), nullptr, x.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 3.2472691535949707,
        "total_tokens": 1468,
        "prompt_tokens": 1295
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "58_transform_squaring",
        "parallelism_model": "serial",
        "prompt": "/* Replace every element of x with the square of its value.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {",
        "outputs": [
            {
                "generated_output": " \nfor (auto &value : x) {\n    value = value * value;\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 8.733095169067383,
        "total_tokens": 1274,
        "prompt_tokens": 1249
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "26_reduce_product_of_inverses",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Use MPI to compute product in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the product on all ranks.\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(std::vector<double> const& x) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nint n = x.size();\ndouble localProduct = 1.0;\n\nfor (int i = rank; i < n; i += size) {\n    if (i % 2 == 0) {\n        localProduct *= x[i];\n    } else {\n        localProduct *= 1.0 / x[i];\n    }\n}\n\ndouble globalProduct;\nMPI_Reduce(&localProduct, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, 0, MPI_COMM_WORLD);\n\nreturn globalProduct;\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": true,
                "are_all_valid": true,
                "best_sequential_runtime": 0.0407242,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.0226552,
                        "num_procs": 1
                    },
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.0128179,
                        "num_procs": 2
                    },
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.0071219,
                        "num_procs": 4
                    }
                ]
            }
        ],
        "response_time": 6.724275827407837,
        "total_tokens": 1493,
        "prompt_tokens": 1317
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "26_reduce_product_of_inverses",
        "parallelism_model": "serial",
        "prompt": "/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(std::vector<double> const& x) {",
        "outputs": [
            {
                "generated_output": " \ndouble result = 1.0;\nfor (size_t i = 0; i < x.size(); ++i) {\n    if (i % 2 == 0) {\n        result *= x[i];\n    } else {\n        result *= 1.0 / x[i];\n    }\n}\nreturn result;\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 4.523665904998779,
        "total_tokens": 1356,
        "prompt_tokens": 1273
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "27_reduce_average",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the average of the vector x.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the average on all ranks.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(std::vector<double> const& x) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\ndouble localSum = 0.0;\nfor (const auto& value : x) {\n    localSum += value;\n}\n\ndouble globalSum;\nMPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\nif (rank == 0) {\n    return globalSum / (x.size() * size);\n}\nreturn 0.0;\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": true,
                "are_all_valid": true,
                "best_sequential_runtime": 0.0229346,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.0237714,
                        "num_procs": 1
                    },
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.0248662,
                        "num_procs": 2
                    },
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.0280282,
                        "num_procs": 4
                    }
                ]
            }
        ],
        "response_time": 10.950135231018066,
        "total_tokens": 1451,
        "prompt_tokens": 1301
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "27_reduce_average",
        "parallelism_model": "serial",
        "prompt": "/* Return the average of the vector x.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(std::vector<double> const& x) {",
        "outputs": [
            {
                "generated_output": " \nif (x.empty()) {\n    return 0.0;\n}\n\ndouble sum = 0.0;\nfor (const auto& value : x) {\n    sum += value;\n}\n\nreturn sum / x.size();\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": true,
                "are_all_valid": true,
                "best_sequential_runtime": 0.0237790168,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.0237843668
                    }
                ]
            }
        ],
        "response_time": 3.1306509971618652,
        "total_tokens": 1321,
        "prompt_tokens": 1258
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "28_reduce_smallest_odd_number",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the value of the smallest odd number in the vector x.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on all ranks.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nint localMin = INT_MAX;\nfor (const auto& num : x) {\n    if (num % 2 != 0 && num < localMin) {\n        localMin = num;\n    }\n}\n\nint globalMin;\nMPI_Reduce(&localMin, &globalMin, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\nif (rank == 0) {\n    return globalMin;\n}\nreturn 0;\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 3.5254859924316406,
        "total_tokens": 1473,
        "prompt_tokens": 1315
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "28_reduce_smallest_odd_number",
        "parallelism_model": "serial",
        "prompt": "/* Return the value of the smallest odd number in the vector x.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {",
        "outputs": [
            {
                "generated_output": " \nint smallestOdd = INT_MAX;\nbool foundOdd = false;\n\n#pragma omp parallel for reduction(min:smallestOdd)\nfor (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 != 0) {\n        if (x[i] < smallestOdd) {\n            smallestOdd = x[i];\n            foundOdd = true;\n        }\n    }\n}\n\nreturn foundOdd ? smallestOdd : -1;\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 3.1072301864624023,
        "total_tokens": 1396,
        "prompt_tokens": 1273
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "25_reduce_xor",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Use MPI to reduce in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on all ranks.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(std::vector<bool> const& x) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nbool localResult = false;\nif (rank == 0) {\n    localResult = x[0];\n}\n\nfor (size_t i = rank; i < x.size(); i += size) {\n    if (rank == 0) {\n        localResult = localResult != x[i];\n    } else {\n        localResult = x[i];\n    }\n}\n\nbool globalResult;\nMPI_Reduce(&localResult, &globalResult, 1, MPI_C_BOOL, MPI_LOR, 0, MPI_COMM_WORLD);\n\nif (rank == 0) {\n    return globalResult;\n}\nreturn false;\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": true,
                "are_all_valid": false,
                "best_sequential_runtime": 0.0,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.0244614,
                        "num_procs": 1
                    },
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 2
                    },
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 4
                    }
                ]
            }
        ],
        "response_time": 4.189304828643799,
        "total_tokens": 1474,
        "prompt_tokens": 1272
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "25_reduce_xor",
        "parallelism_model": "serial",
        "prompt": "/* Return the logical XOR reduction of the vector of bools x.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(std::vector<bool> const& x) {",
        "outputs": [
            {
                "generated_output": " \nbool result = false;\n#pragma omp parallel for reduction(^:result)\nfor (size_t i = 0; i < x.size(); ++i) {\n    result ^= x[i];\n}\nreturn result;\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": true,
                "are_all_valid": true,
                "best_sequential_runtime": 9.14e-08,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.0063309668
                    }
                ]
            }
        ],
        "response_time": 3.8966667652130127,
        "total_tokens": 1292,
        "prompt_tokens": 1230
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "29_reduce_sum_of_min_of_pairs",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Use MPI to sum in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Return the sum on all ranks.\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ndouble sumOfMinimumElements(std::vector<double> const& x, std::vector<double> const& y) {",
        "outputs": [
            {
                "generated_output": " \n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double localSum = 0.0;\n    for (size_t i = rank; i < x.size(); i += size) {\n        localSum += std::min(x[i], y[i]);\n    }\n\n    double globalSum = 0.0;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalSum;\n    }\n    return 0.0;\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": true,
                "are_all_valid": true,
                "best_sequential_runtime": 0.050757,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.0306552,
                        "num_procs": 1
                    },
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.014158,
                        "num_procs": 2
                    },
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.02183265,
                        "num_procs": 4
                    }
                ]
            }
        ],
        "response_time": 4.545431852340698,
        "total_tokens": 1536,
        "prompt_tokens": 1358
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "29_reduce_sum_of_min_of_pairs",
        "parallelism_model": "serial",
        "prompt": "/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ndouble sumOfMinimumElements(std::vector<double> const& x, std::vector<double> const& y) {",
        "outputs": [
            {
                "generated_output": " \nif (x.size() != y.size()) {\n    throw std::invalid_argument(\"Vectors x and y must have the same size\");\n}\n\ndouble sum = 0.0;\n#pragma omp parallel for reduction(+:sum)\nfor (size_t i = 0; i < x.size(); ++i) {\n    sum += std::min(x[i], y[i]);\n}\n\nreturn sum;\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": true,
                "are_all_valid": true,
                "best_sequential_runtime": 0.0489436498,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.0265840918
                    }
                ]
            }
        ],
        "response_time": 3.336132049560547,
        "total_tokens": 1421,
        "prompt_tokens": 1313
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "00_dense_la_lu_decomp",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. Store the result in A on rank 0.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\nvoid luFactorize(std::vector<double> &A, size_t N) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nfor (size_t k = 0; k < N; ++k) {\n    if (rank == 0) {\n        for (int i = k + 1; i < N; ++i) {\n            A[i * N + k] /= A[k * N + k];\n        }\n    }\n    MPI_Bcast(&A[k * N + k], N - k, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = k + 1; i < N; ++i) {\n        if (rank == 0) {\n            for (int j = k + 1; j < N; ++j) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n        MPI_Bcast(&A[i * N + k], N - k, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n}\n\nif (rank != 0) {\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (i > j) {\n                A[i * N + j] = 0.0;\n            }\n        }\n    }\n}\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": true,
                "are_all_valid": true,
                "best_sequential_runtime": 0.1531278,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.191588,
                        "num_procs": 1
                    },
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 1.388039,
                        "num_procs": 2
                    },
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 2.7012884,
                        "num_procs": 4
                    }
                ]
            }
        ],
        "response_time": 5.227386951446533,
        "total_tokens": 1703,
        "prompt_tokens": 1339
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "00_dense_la_lu_decomp",
        "parallelism_model": "serial",
        "prompt": "/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\nvoid luFactorize(std::vector<double> &A, size_t N) {",
        "outputs": [
            {
                "generated_output": " \nfor (size_t i = 0; i < N; ++i) {\n    // Update the diagonal element of U\n    for (size_t k = i; k < N; ++k) {\n        double sum = 0;\n        for (size_t j = 0; j < i; ++j) {\n            sum += A[i * N + j] * A[j * N + k];\n        }\n        A[i * N + k] -= sum;\n    }\n\n    // Update the elements below the diagonal in L\n    for (size_t k = i + 1; k < N; ++k) {\n        double sum = 0;\n        for (size_t j = 0; j < i; ++j) {\n            sum += A[k * N + j] * A[j * N + i];\n        }\n        A[k * N + i] = (A[k * N + i] - sum) / A[i * N + i];\n    }\n}\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null
                    }
                ]
            }
        ],
        "response_time": 4.340252161026001,
        "total_tokens": 1538,
        "prompt_tokens": 1293
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "04_dense_la_gemv",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and x. Store the result in y on rank 0.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(std::vector<double> const& A, std::vector<double> const& x, std::vector<double> &y, size_t M, size_t N) {",
        "outputs": [
            {
                "generated_output": " \n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<double> local_y(M / size, 0.0);\n\n    for (size_t i = rank; i < M; i += size) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        local_y[i / size] = sum;\n    }\n\n    std::vector<double> global_y(M);\n    MPI_Allgather(local_y.data(), M / size, MPI_DOUBLE, global_y.data(), M / size, MPI_DOUBLE, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        y = global_y;\n    }\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": true,
                "are_all_valid": false,
                "best_sequential_runtime": 0.0433954,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.0438184,
                        "num_procs": 1
                    },
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 2
                    },
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 4
                    }
                ]
            }
        ],
        "response_time": 4.566843032836914,
        "total_tokens": 1600,
        "prompt_tokens": 1362
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "04_dense_la_gemv",
        "parallelism_model": "serial",
        "prompt": "/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(std::vector<double> const& A, std::vector<double> const& x, std::vector<double> &y, size_t M, size_t N) {",
        "outputs": [
            {
                "generated_output": " \nfor (size_t i = 0; i < M; ++i) {\n    y[i] = 0.0;\n    for (size_t j = 0; j < N; ++j) {\n        y[i] += A[i * N + j] * x[j];\n    }\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 2.5147671699523926,
        "total_tokens": 1395,
        "prompt_tokens": 1314
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "02_dense_la_gemm",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and B. Store the result in C on rank 0.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C, size_t M, size_t K, size_t N) {",
        "outputs": [
            {
                "generated_output": " \n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Each rank will handle a portion of the matrix A\n    size_t local_M = M / size;\n    size_t remainder = M % size;\n    size_t start_row = rank * local_M + (rank < remainder ? rank : remainder);\n    size_t end_row = start_row + local_M + (rank < remainder ? 1 : 0);\n\n    // Each rank will handle a portion of the matrix B\n    size_t local_K = K / size;\n    remainder = K % size;\n    size_t start_col = rank * local_K + (rank < remainder ? rank : remainder);\n    size_t end_col = start_col + local_K + (rank < remainder ? 1 : 0);\n\n    // Perform local matrix multiplication\n    for (size_t i = start_row; i < end_row; ++i) {\n        for (size_t j = start_col; j < end_col; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n\n    // Gather results on rank 0\n    if (rank == 0) {\n        for (int src = 1; src < size; ++src) {\n            for (size_t i = src * local_M; i < (src + 1) * local_M + (src < remainder ? 1 : 0); ++i) {\n                for (size_t j = 0; j < N; ++j) {\n                    MPI_Recv(&C[i * N + j], 1, MPI_DOUBLE, src, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n                }\n            }\n        }\n    } else {\n        for (size_t i = start_row; i < end_row; ++i) {\n            for (size_t j = start_col; j < end_col; ++j) {\n                MPI_Send(&C[i * N + j], 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n            }\n        }\n    }\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": false,
                "are_any_valid": true,
                "are_all_valid": false,
                "best_sequential_runtime": 0.0039886,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.010534,
                        "num_procs": 1
                    },
                    {
                        "did_run": false,
                        "is_valid": null,
                        "runtime": null,
                        "num_procs": 2
                    },
                    {
                        "did_run": false,
                        "is_valid": null,
                        "runtime": null,
                        "num_procs": 4
                    }
                ]
            }
        ],
        "response_time": 8.198349952697754,
        "total_tokens": 1990,
        "prompt_tokens": 1392
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "02_dense_la_gemm",
        "parallelism_model": "serial",
        "prompt": "/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C, size_t M, size_t K, size_t N) {",
        "outputs": [
            {
                "generated_output": " \nfor (size_t i = 0; i < M; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        double sum = 0.0;\n        for (size_t k = 0; k < K; ++k) {\n            sum += A[i * K + k] * B[k * N + j];\n        }\n        C[i * N + j] = sum;\n    }\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 3.712630033493042,
        "total_tokens": 1460,
        "prompt_tokens": 1344
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "01_dense_la_solve",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and b. Store the result in x on rank 0.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
        "outputs": [
            {
                "generated_output": " \n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Each process works on its own part of the matrix and vector\n    std::vector<double> localA(N / size * N);\n    std::vector<double> localB(N / size);\n    std::vector<double> localX(N / size);\n\n    // Scatter matrix A and vector b to all processes\n    MPI_Scatter(A.data(), N / size * N, MPI_DOUBLE, localA.data(), N / size * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Scatter(b.data(), N / size, MPI_DOUBLE, localB.data(), N / size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // Solve the local linear system\n    for (size_t i = 0; i < N / size; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += localA[i * N + j];\n        }\n        localX[i] = localB[i] / sum;\n    }\n\n    // Gather the results back to rank 0\n    MPI_Gather(localX.data(), N / size, MPI_DOUBLE, x.data(), N / size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 1
                    },
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 2
                    },
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 4
                    }
                ]
            }
        ],
        "response_time": 5.9701831340789795,
        "total_tokens": 1721,
        "prompt_tokens": 1356
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "01_dense_la_solve",
        "parallelism_model": "serial",
        "prompt": "/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
        "outputs": [
            {
                "generated_output": " \nif (A.size() != N * N || b.size() != N || x.size() != N) {\n    throw std::invalid_argument(\"Input sizes do not match.\");\n}\n\n// Use Gaussian elimination with partial pivoting\nfor (size_t i = 0; i < N; ++i) {\n    // Partial pivoting\n    size_t maxRow = i;\n    for (size_t k = i + 1; k < N; ++k) {\n        if (std::abs(A[k * N + i]) > std::abs(A[maxRow * N + i])) {\n            maxRow = k;\n        }\n    }\n\n    // Swap rows\n    for (size_t k = i; k < N; ++k) {\n        std::swap(A[i * N + k], A[maxRow * N + k]);\n    }\n    std::swap(b[i], b[maxRow]);\n\n    // Make the diagonal element 1 and eliminate the column\n    double pivot = A[i * N + i];\n    if (pivot == 0) {\n        throw std::runtime_error(\"Matrix is singular.\");\n    }\n    for (size_t k = i; k < N; ++k) {\n        A[i * N + k] /= pivot;\n    }\n    b[i] /= pivot;\n\n    // Eliminate the current column elements in all rows below the pivot\n    for (size_t k = i + 1; k < N; ++k) {\n        double factor = A[k * N + i];\n        for (size_t j = i; j < N; ++j) {\n            A[k * N + j] -= factor * A[i * N + j];\n        }\n        b[k] -= factor * b[i];\n    }\n}\n\n// Back substitution\nfor (size_t i = N; i-- > 0;) {\n    x[i] = b[i];\n    for (size_t k = i + 1; k < N; ++k) {\n        x[i] -= A[i * N + k] * x[k];\n    }\n}\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 7.211446046829224,
        "total_tokens": 1825,
        "prompt_tokens": 1308
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "03_dense_la_axpy",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Store the result in z on rank 0.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, std::vector<double> const& x, std::vector<double> const& y, std::vector<double> &z) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nif (rank == 0) {\n    z.resize(x.size());\n}\n\nstd::vector<double> local_z(x.size(), 0.0);\n\nfor (size_t i = rank; i < x.size(); i += size) {\n    local_z[i] = alpha * x[i] + y[i];\n}\n\nMPI_Reduce(&local_z[0], &z[0], x.size(), MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": true,
                "are_all_valid": true,
                "best_sequential_runtime": 0.008319,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.0410182,
                        "num_procs": 1
                    },
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.1039244,
                        "num_procs": 2
                    },
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.37057175,
                        "num_procs": 4
                    }
                ]
            }
        ],
        "response_time": 3.1661219596862793,
        "total_tokens": 1513,
        "prompt_tokens": 1340
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "03_dense_la_axpy",
        "parallelism_model": "serial",
        "prompt": "/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, std::vector<double> const& x, std::vector<double> const& y, std::vector<double> &z) {",
        "outputs": [
            {
                "generated_output": " \nif (x.size() != y.size() || x.size() != z.size()) {\n    throw std::invalid_argument(\"Vectors x, y, and z must have the same size\");\n}\n\n#pragma omp parallel for\nfor (size_t i = 0; i < x.size(); ++i) {\n    z[i] = alpha * x[i] + y[i];\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 2.2693328857421875,
        "total_tokens": 1394,
        "prompt_tokens": 1292
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "41_sort_k-th_smallest_element",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Find the k-th smallest element of the vector x.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nint n = x.size();\n\n// Each rank sends its local k-th smallest element to rank 0\nstd::vector<int> localKthSmallest(size, 0);\nMPI_Allgather(&x[rank * (n / size)], n / size, MPI_INT, localKthSmallest.data(), n / size, MPI_INT, MPI_COMM_WORLD);\n\n// Sort the local k-th smallest elements\nstd::sort(localKthSmallest.begin() + rank * (n / size), localKthSmallest.begin() + (rank + 1) * (n / size));\n\n// Gather all local k-th smallest elements into a single array on rank 0\nstd::vector<int> allKthSmallest(n);\nMPI_Gather(localKthSmallest.data() + rank * (n / size), n / size, MPI_INT, allKthSmallest.data(), n / size, MPI_INT, 0, MPI_COMM_WORLD);\n\nif (rank == 0) {\n    // Sort the gathered elements to find the global k-th smallest element\n    std::sort(allKthSmallest.begin(), allKthSmallest.end());\n    return allKthSmallest[k];\n}\n\nreturn 0;\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": [
                    {
                        "did_run": false,
                        "is_valid": null,
                        "runtime": null,
                        "num_procs": 1
                    },
                    {
                        "did_run": false,
                        "is_valid": null,
                        "runtime": null,
                        "num_procs": 2
                    },
                    {
                        "did_run": false,
                        "is_valid": null,
                        "runtime": null,
                        "num_procs": 4
                    }
                ]
            }
        ],
        "response_time": 5.031097173690796,
        "total_tokens": 1645,
        "prompt_tokens": 1299
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "41_sort_k-th_smallest_element",
        "parallelism_model": "serial",
        "prompt": "/* Find the k-th smallest element of the vector x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {",
        "outputs": [
            {
                "generated_output": " \n#include <vector>\n#include <algorithm>\n\nint findKthSmallest(std::vector<int> const& x, int k) {\n    if (x.empty() || k <= 0 || k > x.size()) {\n        return -1; // Invalid input\n    }\n\n    std::vector<int> temp = x;\n    std::nth_element(temp.begin(), temp.begin() + k - 1, temp.end());\n    return temp[k - 1];\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 4.350913047790527,
        "total_tokens": 1379,
        "prompt_tokens": 1255
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "43_sort_sort_an_array_of_structs_by_key",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use MPI to sort in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of results. Store the output in results on rank 0.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {",
        "outputs": [
            {
                "generated_output": " \nMPI_Comm comm = MPI_COMM_WORLD;\nint rank, size;\nMPI_Comm_rank(comm, &rank);\nMPI_Comm_size(comm, &size);\n\n// Each rank has a complete copy of results, so we need to sort locally\nstd::sort(results.begin(), results.end(), [](const Result& a, const Result& b) {\n    return a.startTime < b.startTime;\n});\n\n// Gather all results to rank 0\nstd::vector<Result> allResults;\nif (rank == 0) {\n    allResults.resize(results.size() * size);\n}\n\nMPI_Gather(results.data(), results.size(), MPI_INT, allResults.data(), results.size(), MPI_INT, 0, comm);\n\nif (rank == 0) {\n    // Sort the gathered results on rank 0\n    std::sort(allResults.begin(), allResults.end(), [](const Result& a, const Result& b) {\n        return a.startTime < b.startTime;\n    });\n\n    // Scatter the sorted results back to each rank\n    MPI_Scatter(allResults.data(), results.size(), MPI_INT, results.data(), results.size(), MPI_INT, 0, comm);\n} else {\n    MPI_Scatter(nullptr, results.size(), MPI_INT, results.data(), results.size(), MPI_INT, 0, comm);\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 5.786118268966675,
        "total_tokens": 1720,
        "prompt_tokens": 1380
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "43_sort_sort_an_array_of_structs_by_key",
        "parallelism_model": "serial",
        "prompt": "struct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {",
        "outputs": [
            {
                "generated_output": " \nstd::sort(results.begin(), results.end(), [](const Result& a, const Result& b) {\n    return a.startTime < b.startTime;\n});\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 2.7653627395629883,
        "total_tokens": 1378,
        "prompt_tokens": 1334
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "44_sort_sort_non-zero_elements",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n\t Use MPI to sort x in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {",
        "outputs": [
            {
                "generated_output": " \n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Gather all non-zero elements to rank 0\n    std::vector<int> nonZeroElements;\n    for (const auto& elem : x) {\n        if (elem != 0) {\n            nonZeroElements.push_back(elem);\n        }\n    }\n\n    // Sort the non-zero elements\n    std::sort(nonZeroElements.begin(), nonZeroElements.end());\n\n    // Scatter the sorted non-zero elements back to all ranks\n    MPI_Scatter(nonZeroElements.data(), nonZeroElements.size() / size, MPI_INT,\n                x.data() + rank * (nonZeroElements.size() / size), nonZeroElements.size() / size, MPI_INT,\n                0, MPI_COMM_WORLD);\n\n    // Fill in the zero elements\n    if (rank == 0) {\n        int zeroCount = 0;\n        for (auto& elem : x) {\n            if (elem == 0) {\n                elem = nonZeroElements[zeroCount++];\n            }\n        }\n    }\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 1
                    },
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 2
                    },
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 4
                    }
                ]
            }
        ],
        "response_time": 5.176817893981934,
        "total_tokens": 1629,
        "prompt_tokens": 1330
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "44_sort_sort_non-zero_elements",
        "parallelism_model": "serial",
        "prompt": "/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place.\n   Example:\n\n\t input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {",
        "outputs": [
            {
                "generated_output": " \nstd::vector<int> filtered;\nfor (int value : x) {\n    if (value != 0) {\n        filtered.push_back(value);\n    }\n}\nstd::sort(filtered.begin(), filtered.end());\n\nsize_t index = 0;\nfor (int value : filtered) {\n    while (x[index] != 0) {\n        ++index;\n    }\n    x[index++] = value;\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 3.7737739086151123,
        "total_tokens": 1395,
        "prompt_tokens": 1283
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "40_sort_sort_an_array_of_complex_numbers_by_magnitude",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use MPI to sort in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {",
        "outputs": [
            {
                "generated_output": " \n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Each process has a copy of the vector, so we need to gather all vectors to rank 0\n    std::vector<std::complex<double>> allData;\n    MPI_Gather(x.data(), x.size(), MPI_CXX_DOUBLE_COMPLEX, nullptr, 0, MPI_CXX_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        // Rank 0 now has all the data\n        std::vector<std::pair<double, std::complex<double>>> magnitudeData;\n        for (const auto& value : allData) {\n            magnitudeData.emplace_back(std::abs(value), value);\n        }\n\n        // Sort the data by magnitude\n        std::sort(magnitudeData.begin(), magnitudeData.end());\n\n        // Extract the sorted data back into the original vector format\n        x.clear();\n        for (const auto& pair : magnitudeData) {\n            x.push_back(pair.second);\n        }\n    } else {\n        // Other ranks just need to gather the data\n        MPI_Gather(x.data(), x.size(), MPI_CXX_DOUBLE_COMPLEX, nullptr, 0, MPI_CXX_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 5.182754039764404,
        "total_tokens": 1724,
        "prompt_tokens": 1372
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "40_sort_sort_an_array_of_complex_numbers_by_magnitude",
        "parallelism_model": "serial",
        "prompt": "/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {",
        "outputs": [
            {
                "generated_output": " \nstd::sort(x.begin(), x.end(), [](const std::complex<double>& a, const std::complex<double>& b) {\n    return std::abs(a) < std::abs(b);\n});\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 2.1305220127105713,
        "total_tokens": 1383,
        "prompt_tokens": 1326
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "42_sort_sorted_ranks",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every process has a complete copy of x. Store the result in ranks on process 0.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {",
        "outputs": [
            {
                "generated_output": " \n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<float> localX = std::vector<float>(x.begin() + rank * (x.size() / size), x.begin() + (rank + 1) * (x.size() / size));\n    std::sort(localX.begin(), localX.end());\n\n    std::vector<size_t> localRanks(localX.size());\n    for (size_t i = 0; i < localX.size(); ++i) {\n        localRanks[i] = std::distance(x.begin(), std::find(x.begin(), x.end(), localX[i]));\n    }\n\n    std::vector<size_t> globalRanks(x.size());\n    MPI_Gather(&localRanks[0], localRanks.size(), MPI_SIZE_T, &globalRanks[0], localRanks.size(), MPI_SIZE_T, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < x.size(); ++i) {\n            ranks[i] = globalRanks[i];\n        }\n    }\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 5.145022869110107,
        "total_tokens": 1701,
        "prompt_tokens": 1378
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "42_sort_sorted_ranks",
        "parallelism_model": "serial",
        "prompt": "/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {",
        "outputs": [
            {
                "generated_output": " \n    if (x.empty()) {\n        return;\n    }\n\n    std::vector<std::pair<float, size_t>> indexed_x;\n    indexed_x.reserve(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        indexed_x.emplace_back(x[i], i);\n    }\n\n    std::sort(indexed_x.begin(), indexed_x.end(), [](const std::pair<float, size_t>& a, const std::pair<float, size_t>& b) {\n        return a.first < b.first;\n    });\n\n    ranks.resize(x.size());\n    for (size_t i = 0; i < indexed_x.size(); ++i) {\n        ranks[indexed_x[i].second] = i;\n    }\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": true,
                "are_all_valid": true,
                "best_sequential_runtime": 0.0159649168,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.012970092
                    }
                ]
            }
        ],
        "response_time": 4.1069629192352295,
        "total_tokens": 1546,
        "prompt_tokens": 1332
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "46_sparse_la_spmm",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and X. Store the result in Y on rank 0.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(std::vector<COOElement> const& A, std::vector<COOElement> const& X, std::vector<double> &Y, size_t M, size_t K, size_t N) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nstd::vector<double> localY(M * N, 0.0);\n\nfor (const auto& a : A) {\n    for (const auto& x : X) {\n        if (a.column == x.row) {\n            localY[a.row * N + x.column] += a.value * x.value;\n        }\n    }\n}\n\nif (rank == 0) {\n    std::vector<double> globalY(M * N, 0.0);\n    MPI_Reduce(&localY[0], &globalY[0], M * N, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n    Y = globalY;\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 3.7835960388183594,
        "total_tokens": 1626,
        "prompt_tokens": 1414
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "46_sparse_la_spmm",
        "parallelism_model": "serial",
        "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix in row-major.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(std::vector<COOElement> const& A, std::vector<COOElement> const& X, std::vector<double> &Y, size_t M, size_t K, size_t N) {",
        "outputs": [
            {
                "generated_output": " \nif (A.empty() || X.empty()) {\n    return;\n}\n\nstd::vector<double> row(N, 0.0);\n\n#pragma omp parallel for\nfor (size_t i = 0; i < A.size(); ++i) {\n    size_t rowA = A[i].row;\n    size_t colA = A[i].column;\n    double valueA = A[i].value;\n\n    for (size_t j = 0; j < X.size(); ++j) {\n        size_t colX = X[j].column;\n        double valueX = X[j].value;\n\n        if (colA == X[j].row) {\n            row[colX] += valueA * valueX;\n        }\n    }\n}\n\nfor (size_t i = 0; i < N; ++i) {\n    Y[A[0].row * N + i] = row[i];\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 4.793009042739868,
        "total_tokens": 1603,
        "prompt_tokens": 1366
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "47_sparse_la_spmv",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   A has dimensions MxN, x has N values, and y has M values.\n   Use MPI to parallelize. Assume MPI has already been initialized.\n   Every rank has a complete copy of A, x, and y. Store the result in y on rank 0.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, std::vector<COOElement> const& A, std::vector<double> const& x, double beta, std::vector<double> &y, size_t M, size_t N) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nstd::vector<double> local_y(M, 0.0);\n\nfor (const auto& elem : A) {\n    if (elem.row < M && elem.column < N) {\n        local_y[elem.row] += elem.value * x[elem.column];\n    }\n}\n\nstd::vector<double> global_y(M, 0.0);\nMPI_Reduce(&local_y[0], &global_y[0], M, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\nif (rank == 0) {\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = alpha * global_y[i] + beta * y[i];\n    }\n}\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": true,
                "are_all_valid": false,
                "best_sequential_runtime": 0.0053722,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.0025738,
                        "num_procs": 1
                    },
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 2
                    },
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 4
                    }
                ]
            }
        ],
        "response_time": 3.989169120788574,
        "total_tokens": 1657,
        "prompt_tokens": 1422
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "47_sparse_la_spmv",
        "parallelism_model": "serial",
        "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   A has dimensions MxN, x has N values, and y has M values.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, std::vector<COOElement> const& A, std::vector<double> const& x, double beta, std::vector<double> &y, size_t M, size_t N) {",
        "outputs": [
            {
                "generated_output": " \nif (A.empty() || x.empty() || y.size() < M) {\n    return;\n}\n\n#pragma omp parallel for\nfor (size_t i = 0; i < M; ++i) {\n    y[i] = beta * y[i];\n}\n\nfor (const auto& element : A) {\n    if (element.row < M && element.column < N) {\n        y[element.row] += alpha * element.value * x[element.column];\n    }\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 3.52585768699646,
        "total_tokens": 1501,
        "prompt_tokens": 1372
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "48_sparse_la_sparse_axpy",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct Element {\n\tsize_t index;\n  double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors. Store the result in the dense vector z.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Store the result in z on rank 0.\n   Example:\n   \n   input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n   output: z=[0, 0, 0, 1, 0, 10, 0, 1, 0, 0, 0, 0, -1]\n*/\nvoid sparseAxpy(double alpha, std::vector<Element> const& x, std::vector<Element> const& y, std::vector<double> &z) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n// Create local copies of x and y on each rank\nstd::vector<Element> local_x = x;\nstd::vector<Element> local_y = y;\n\n// Scatter x and y to all ranks\nMPI_Bcast(&local_x[0], local_x.size(), MPI_INT, 0, MPI_COMM_WORLD);\nMPI_Bcast(&local_y[0], local_y.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n// Compute local Axpy\nfor (const auto& elem : local_x) {\n    z[elem.index] += alpha * elem.value;\n}\nfor (const auto& elem : local_y) {\n    z[elem.index] += elem.value;\n}\n\n// Gather results from all ranks to rank 0\nMPI_Gather(&z[0], z.size(), MPI_DOUBLE, &z[0], z.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 4.25885009765625,
        "total_tokens": 1688,
        "prompt_tokens": 1412
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "48_sparse_la_sparse_axpy",
        "parallelism_model": "serial",
        "prompt": "struct Element {\n\tsize_t index;\n  double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors. Store the result in the dense vector z.\n   Example:\n   \n   input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n   output: z=[0, 0, 0, 1, 0, 10, 0, 1, 0, 0, 0, 0, -1]\n*/\nvoid sparseAxpy(double alpha, std::vector<Element> const& x, std::vector<Element> const& y, std::vector<double> &z) {",
        "outputs": [
            {
                "generated_output": " \nif (x.empty() && y.empty()) {\n    return;\n}\n\n#pragma omp parallel for\nfor (size_t i = 0; i < z.size(); ++i) {\n    z[i] = 0.0;\n}\n\nfor (const auto& elem : x) {\n    z[elem.index] += alpha * elem.value;\n}\n\nfor (const auto& elem : y) {\n    z[elem.index] += elem.value;\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 3.889806032180786,
        "total_tokens": 1487,
        "prompt_tokens": 1364
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "45_sparse_la_sparse_solve",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format. x and b are dense vectors with N elements.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and b. Store the result in x on rank 0.\n   Example:\n   \n   input: A=[{0,0,1}, {0,1,1}, {1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\nvoid solveLinearSystem(std::vector<COOElement> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n// Distribute the matrix A and vector b to all ranks\nstd::vector<double> localA;\nstd::vector<double> localB;\nstd::vector<double> localX;\n\n// Calculate the size of local data\nsize_t localRows = N / size;\nif (rank < N % size) {\n    localRows++;\n}\n\nlocalA.reserve(localRows * N);\nlocalB.reserve(localRows);\nlocalX.reserve(N);\n\nfor (const auto& elem : A) {\n    if (elem.row >= rank * localRows && elem.row < (rank + 1) * localRows) {\n        localA.push_back(elem.value);\n    }\n    if (elem.row == rank * localRows) {\n        localB.push_back(elem.value);\n    }\n}\n\n// Perform local matrix-vector multiplication\ndouble localDotProduct = 0.0;\nfor (size_t i = 0; i < localA.size(); i += N) {\n    for (size_t j = 0; j < N; ++j) {\n        localDotProduct += localA[i + j] * b[j];\n    }\n}\n\n// Reduce the local dot products to get the final result on rank 0\nstd::vector<double> globalDotProduct(size);\nMPI_Allgather(&localDotProduct, 1, MPI_DOUBLE, globalDotProduct.data(), 1, MPI_DOUBLE, MPI_COMM_WORLD);\n\nif (rank == 0) {\n    double sum = 0.0;\n    for (const auto& val : globalDotProduct) {\n        sum += val;\n    }\n    x[0] = sum;\n}\n\n}",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 1
                    },
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 2
                    },
                    {
                        "did_run": true,
                        "is_valid": null,
                        "runtime": null,
                        "num_procs": 4
                    }
                ]
            }
        ],
        "response_time": 6.754885196685791,
        "total_tokens": 1828,
        "prompt_tokens": 1376
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "45_sparse_la_sparse_solve",
        "parallelism_model": "serial",
        "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format. x and b are dense vectors with N elements.\n   Example:\n   \n   input: A=[{0,0,1}, {0,1,1}, {1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\nvoid solveLinearSystem(std::vector<COOElement> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
        "outputs": [
            {
                "generated_output": " \nif (A.empty() || b.empty() || A.size() != b.size()) {\n    return;\n}\n\nx.resize(N, 0.0);\n\nfor (const auto& element : A) {\n    if (element.row < N && element.column < N) {\n        x[element.row] -= element.value * b[element.column];\n    }\n}\n\nfor (auto& value : x) {\n    value /= A[0].value; // Assuming A[0].value is the diagonal element (simplified)\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 3.437001943588257,
        "total_tokens": 1466,
        "prompt_tokens": 1328
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "49_sparse_la_sparse_lu_decomp",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. Store the result in L and U on rank 0.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(std::vector<COOElement> const& A, std::vector<double> &L, std::vector<double> &U, size_t N) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n// Distribute the matrix A to all ranks\nstd::vector<COOElement> localA;\nfor (const auto& elem : A) {\n    if (elem.row < rank * (N / size) || elem.row >= (rank + 1) * (N / size)) {\n        continue;\n    }\n    localA.push_back(elem);\n}\n\n// Perform LU factorization on the local matrix\nfor (size_t i = 0; i < localA.size(); ++i) {\n    size_t row = localA[i].row;\n    size_t col = localA[i].column;\n    double val = localA[i].value;\n\n    if (row == col) {\n        // Diagonal element\n        U.push_back({row, col, val});\n        for (size_t k = 0; k < localA.size(); ++k) {\n            if (localA[k].row == row && localA[k].column < col) {\n                L.push_back({localA[k].row, localA[k].column, localA[k].value / val});\n            }\n        }\n    } else if (row > col) {\n        // Lower triangular part\n        L.push_back({row, col, val});\n    } else {\n        // Upper triangular part\n        U.push_back({row, col, val});\n    }\n}\n\n// Gather the results from all ranks\nif (rank == 0) {\n    std::vector<COOElement> globalL, globalU;\n    globalL.reserve(L.size());\n    globalU.reserve(U.size());\n\n    for (int src = 0; src < size; ++src) {\n        std::vector<COOElement> tmpL, tmpU;\n        MPI_Recv(tmpL.data(), tmpL.size(), MPI_COOElement, src, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        MPI_Recv(tmpU.data(), tmpU.size(), MPI_COOElement, src, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        globalL.insert(globalL.end(), tmpL.begin(), tmpL.end());\n        globalU.insert(globalU.end(), tmpU.begin(), tmpU.end());\n    }\n\n    L = std::move(globalL);\n    U = std::move(globalU);\n} else {\n    MPI_Send(L.data(), L.size(), MPI_COOElement, 0, 0, MPI_COMM_WORLD);\n    MPI_Send(U.data(), U.size(), MPI_COOElement, 0, 0, MPI_COMM_WORLD);\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 8.562000036239624,
        "total_tokens": 2115,
        "prompt_tokens": 1422
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "49_sparse_la_sparse_lu_decomp",
        "parallelism_model": "serial",
        "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(std::vector<COOElement> const& A, std::vector<double> &L, std::vector<double> &U, size_t N) {",
        "outputs": [
            {
                "generated_output": " \n    L.resize(A.size());\n    U.resize(A.size());\n    std::vector<double> LU(N * N, 0.0);\n\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += LU[i * N + j] * LU[j * N + k];\n            }\n            LU[i * N + k] = A[i * N + k] - sum;\n        }\n\n        for (size_t k = i; k < N; ++k) {\n            if (i == k) {\n                LU[i * N + i] = 1.0;\n            } else {\n                double sum = 0.0;\n                for (size_t j = 0; j < i; ++j) {\n                    sum += LU[k * N + j] * LU[j * N + i];\n                }\n                LU[k * N + i] = (A[k * N + i] - sum) / LU[i * N + i];\n            }\n        }\n    }\n\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (i > j) {\n                L[i * N + j] = LU[i * N + j];\n                U[i * N + j] = 0.0;\n            } else {\n                L[i * N + j] = 0.0;\n                U[i * N + j] = LU[i * N + j];\n            }\n        }\n    }\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 7.390938997268677,
        "total_tokens": 1818,
        "prompt_tokens": 1374
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "50_stencil_xor_kernel",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
        "outputs": [
            {
                "generated_output": " \n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Each process handles a block of rows\n    size_t rows_per_process = N / size;\n    size_t remainder = N % size;\n    size_t start_row = rank * rows_per_process;\n    if (rank < remainder) {\n        start_row += rank;\n        rows_per_process += 1;\n    } else {\n        start_row += remainder;\n    }\n    size_t end_row = start_row + rows_per_process;\n\n    // Create local copies of the input and output for each process\n    std::vector<int> local_input(input.begin() + start_row * N, input.begin() + end_row * N);\n    std::vector<int> local_output(rows_per_process * N, 0);\n\n    // Compute the local output\n    for (size_t i = 0; i < rows_per_process; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(start_row + i - 1) * N + j] == 1) count++;\n            if (i < rows_per_process - 1 && input[(start_row + i + 1) * N + j] == 1) count++;\n            if (j > 0 && input[(start_row + i) * N + j - 1] == 1) count++;\n            if (j < N - 1 && input[(start_row + i) * N + j + 1] == 1) count++;\n            local_output[i * N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n\n    // Gather the results on rank 0\n    if (rank == 0) {\n        std::vector<int> global_output(N * N, 0);\n        MPI_Gather(local_output.data(), rows_per_process * N, MPI_INT, global_output.data(), rows_per_process * N, MPI_INT, 0, MPI_COMM_WORLD);\n        output = global_output;\n    } else {\n        MPI_Gather(local_output.data(), rows_per_process * N, MPI_INT, nullptr, 0, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": true,
                "are_all_valid": false,
                "best_sequential_runtime": 0.0578158,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.0671046,
                        "num_procs": 1
                    },
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 2
                    },
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 4
                    }
                ]
            }
        ],
        "response_time": 8.211261749267578,
        "total_tokens": 2060,
        "prompt_tokens": 1446
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "50_stencil_xor_kernel",
        "parallelism_model": "serial",
        "prompt": "/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
        "outputs": [
            {
                "generated_output": " \nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        int count = 0;\n        if (i > 0 && input[(i - 1) * N + j] == 1) count++;\n        if (i < N - 1 && input[(i + 1) * N + j] == 1) count++;\n        if (j > 0 && input[i * N + (j - 1)] == 1) count++;\n        if (j < N - 1 && input[i * N + (j + 1)] == 1) count++;\n        output[i * N + j] = (count == 1) ? 1 : 0;\n    }\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 3.0879430770874023,
        "total_tokens": 1592,
        "prompt_tokens": 1399
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "51_stencil_edge_kernel",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nconst int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of imageIn. The result is stored in imageOut on rank 0.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {",
        "outputs": [
            {
                "generated_output": " \n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t localN = N / size;\n    size_t remainder = N % size;\n    size_t start = rank * localN;\n    if (rank == size - 1) {\n        localN += remainder;\n        start += remainder;\n    }\n\n    std::vector<int> localImageIn(localN * N);\n    std::vector<int> localImageOut(localN * N);\n\n    for (size_t i = 0; i < localN; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            localImageIn[i * N + j] = imageIn[(start + i) * N + j];\n        }\n    }\n\n    for (size_t i = 0; i < localN; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int newValue = 0;\n            for (int di = -1; di <= 1; ++di) {\n                for (int dj = -1; dj <= 1; ++dj) {\n                    int ni = static_cast<int>(i) + di;\n                    int nj = static_cast<int>(j) + dj;\n                    if (ni >= 0 && ni < static_cast<int>(localN) && nj >= 0 && nj < static_cast<int>(N)) {\n                        newValue += edgeKernel[di + 1][dj + 1] * localImageIn[ni * N + nj];\n                    }\n                }\n            }\n            localImageOut[i * N + j] = std::max(0, std::min(255, newValue));\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; ++r) {\n            size_t rStart = r * (N / size);\n            if (r == size - 1) {\n                rStart += N % size;\n            }\n            std::vector<int> recvBuffer(N * (N / size));\n            MPI_Recv(recvBuffer.data(), N * (N / size), MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t i = 0; i < N / size; ++i) {\n                for (size_t j = 0; j < N; ++j) {\n                    imageOut[(rStart + i) * N + j] = recvBuffer[i * N + j];\n                }\n            }\n        }\n        for (size_t i = 0; i < localN; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                imageOut[start * N + j] = localImageOut[i * N + j];\n            }\n        }\n    } else {\n        MPI_Send(localImageOut.data(), localN * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 1
                    },
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 2
                    },
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 4
                    }
                ]
            }
        ],
        "response_time": 9.233819246292114,
        "total_tokens": 2326,
        "prompt_tokens": 1557
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "51_stencil_edge_kernel",
        "parallelism_model": "serial",
        "prompt": "const int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {",
        "outputs": [
            {
                "generated_output": " \nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        int sum = 0;\n        for (int di = -1; di <= 1; ++di) {\n            for (int dj = -1; dj <= 1; ++dj) {\n                int ni = static_cast<int>(i) + di;\n                int nj = static_cast<int>(j) + dj;\n                if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                    sum += imageIn[ni * N + nj] * edgeKernel[di + 1][dj + 1];\n                }\n            }\n        }\n        sum = std::max(0, std::min(sum, 255));\n        imageOut[i * N + j] = sum;\n    }\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 4.347024917602539,
        "total_tokens": 1733,
        "prompt_tokens": 1508
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "53_stencil_2d_jacobi_5-point_stencil",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(std::vector<double> const& input, std::vector<double> &output, size_t N) {",
        "outputs": [
            {
                "generated_output": " \n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Each process will handle a subgrid of size N/size x N/size\n    size_t subgrid_size = N / size;\n\n    // Temporary buffers to hold the new values for each process\n    std::vector<double> sub_input(subgrid_size * subgrid_size, 0.0);\n    std::vector<double> sub_output(subgrid_size * subgrid_size, 0.0);\n\n    // Scatter the input grid to all processes\n    MPI_Scatter(input.data(), subgrid_size * subgrid_size, MPI_DOUBLE, sub_input.data(), subgrid_size * subgrid_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // Perform the Jacobi iteration on the subgrid\n    for (size_t i = 0; i < subgrid_size; ++i) {\n        for (size_t j = 0; j < subgrid_size; ++j) {\n            size_t idx = i * subgrid_size + j;\n            double sum = 0.0;\n            if (i > 0) sum += sub_input[(i - 1) * subgrid_size + j];\n            if (i < subgrid_size - 1) sum += sub_input[(i + 1) * subgrid_size + j];\n            if (j > 0) sum += sub_input[i * subgrid_size + (j - 1)];\n            if (j < subgrid_size - 1) sum += sub_input[i * subgrid_size + (j + 1)];\n            sub_output[idx] = (sum + sub_input[idx]) / 5.0;\n        }\n    }\n\n    // Gather the results back to the root process\n    MPI_Gather(sub_output.data(), subgrid_size * subgrid_size, MPI_DOUBLE, output.data(), subgrid_size * subgrid_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": true,
                "are_all_valid": false,
                "best_sequential_runtime": 0.0271702,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.08139,
                        "num_procs": 1
                    },
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 2
                    },
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 4
                    }
                ]
            }
        ],
        "response_time": 7.455260992050171,
        "total_tokens": 2001,
        "prompt_tokens": 1480
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "53_stencil_2d_jacobi_5-point_stencil",
        "parallelism_model": "serial",
        "prompt": "/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(std::vector<double> const& input, std::vector<double> &output, size_t N) {",
        "outputs": [
            {
                "generated_output": " \nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        double sum = 0.0;\n        size_t count = 0;\n\n        // Top-left corner\n        if (i > 0 && j > 0) {\n            sum += input[(i - 1) * N + (j - 1)];\n            count++;\n        }\n        // Top-middle\n        if (i > 0) {\n            sum += input[(i - 1) * N + j];\n            count++;\n        }\n        // Top-right corner\n        if (i > 0 && j < N - 1) {\n            sum += input[(i - 1) * N + (j + 1)];\n            count++;\n        }\n        // Middle-left\n        if (j > 0) {\n            sum += input[i * N + (j - 1)];\n            count++;\n        }\n        // Middle-right\n        if (j < N - 1) {\n            sum += input[i * N + (j + 1)];\n            count++;\n        }\n        // Bottom-left corner\n        if (i < N - 1 && j > 0) {\n            sum += input[(i + 1) * N + (j - 1)];\n            count++;\n        }\n        // Bottom-middle\n        if (i < N - 1) {\n            sum += input[(i + 1) * N + j];\n            count++;\n        }\n        // Bottom-right corner\n        if (i < N - 1 && j < N - 1) {\n            sum += input[(i + 1) * N + (j + 1)];\n            count++;\n        }\n\n        output[i * N + j] = sum / count;\n    }\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 7.239788055419922,
        "total_tokens": 1883,
        "prompt_tokens": 1433
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "52_stencil_1d_jacobi_3-point_stencil",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(std::vector<double> const& input, std::vector<double> &output) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nint n = input.size();\nint local_n = n / size;\n\nstd::vector<double> local_input(local_n + 2);\nstd::vector<double> local_output(local_n + 2);\n\n// Scatter input to all ranks\nMPI_Scatter(input.data(), local_n, MPI_DOUBLE, local_input.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n// Add boundary values\nlocal_input[0] = (rank > 0) ? input[std::max(0, rank * local_n - 1)] : 0;\nlocal_input[local_n + 1] = (rank < size - 1) ? input[(rank + 1) * local_n] : 0;\n\n// Compute local jacobi iteration\nfor (int i = 1; i <= local_n; ++i) {\n    local_output[i] = (local_input[i - 1] + local_input[i] + local_input[i + 1]) / 3.0;\n}\n\n// Gather results on rank 0\nMPI_Gather(local_output.data() + 1, local_n, MPI_DOUBLE, output.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\nif (rank == 0) {\n    // Handle boundary values on rank 0\n    output[0] = (input[0] + input[1]) / 3.0;\n    output[n - 1] = (input[n - 2] + input[n - 1]) / 3.0;\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 6.145175933837891,
        "total_tokens": 1831,
        "prompt_tokens": 1397
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "52_stencil_1d_jacobi_3-point_stencil",
        "parallelism_model": "serial",
        "prompt": "/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(std::vector<double> const& input, std::vector<double> &output) {",
        "outputs": [
            {
                "generated_output": " \nif (input.size() != output.size()) {\n    return;\n}\n\nint n = input.size();\n#pragma omp parallel for\nfor (int i = 0; i < n; ++i) {\n    int left = (i > 0) ? i - 1 : 0;\n    int right = (i < n - 1) ? i + 1 : n - 1;\n    output[i] = (input[left] + input[i] + input[right]) / 3.0;\n}\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": true,
                "are_all_valid": true,
                "best_sequential_runtime": 0.0195711416,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.020076
                    }
                ]
            }
        ],
        "response_time": 4.468524932861328,
        "total_tokens": 1483,
        "prompt_tokens": 1350
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "54_stencil_game_of_life",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
        "outputs": [
            {
                "generated_output": " \n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    const int neighbors[8][2] = {{-1, -1}, {-1, 0}, {-1, 1}, {0, -1}, {0, 1}, {1, -1}, {1, 0}, {1, 1}};\n\n    for (int gen = 0; gen < 10; ++gen) {\n        std::vector<int> new_state(N * N);\n\n        #pragma omp parallel for collapse(2)\n        for (size_t i = rank; i < N; i += size) {\n            for (size_t j = 0; j < N; ++j) {\n                int live_neighbors = 0;\n\n                for (const auto& neighbor : neighbors) {\n                    int ni = static_cast<int>(i) + neighbor[0];\n                    int nj = static_cast<int>(j) + neighbor[1];\n\n                    if (ni >= 0 && ni < static_cast<int>(N) && nj >= 0 && nj < static_cast<int>(N)) {\n                        live_neighbors += input[ni * N + nj];\n                    }\n                }\n\n                if (input[i * N + j] == 1) {\n                    if (live_neighbors < 2 || live_neighbors > 3) {\n                        new_state[i * N + j] = 0;\n                    } else {\n                        new_state[i * N + j] = 1;\n                    }\n                } else {\n                    if (live_neighbors == 3) {\n                        new_state[i * N + j] = 1;\n                    } else {\n                        new_state[i * N + j] = 0;\n                    }\n                }\n            }\n        }\n\n        MPI_Barrier(MPI_COMM_WORLD);\n        MPI_Allreduce(new_state.data(), output.data(), N * N, MPI_INT, MPI_MAX, MPI_COMM_WORLD);\n        input = output;\n    }\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 8.124150037765503,
        "total_tokens": 2089,
        "prompt_tokens": 1566
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "54_stencil_game_of_life",
        "parallelism_model": "serial",
        "prompt": "/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
        "outputs": [
            {
                "generated_output": " \nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        int liveNeighbors = 0;\n\n        // Check all 8 neighbors\n        for (int dx = -1; dx <= 1; ++dx) {\n            for (int dy = -1; dy <= 1; ++dy) {\n                if (dx == 0 && dy == 0) continue;\n                int ni = static_cast<int>(i) + dx;\n                int nj = static_cast<int>(j) + dy;\n                if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                    liveNeighbors += input[ni * N + nj];\n                }\n            }\n        }\n\n        // Apply Game of Life rules\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 5.527437210083008,
        "total_tokens": 1861,
        "prompt_tokens": 1520
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_pixel_histogram",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use MPI to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of image. The result is stored in bins on rank 0.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n// Each rank counts the number of pixels with each intensity\nstd::array<size_t, 256> localBins = {0};\nfor (int pixel : image) {\n    localBins[pixel]++;\n}\n\n// Reduce the local bins to get the global counts\nMPI_Reduce(&localBins[0], &bins[0], 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": true,
                "are_all_valid": false,
                "best_sequential_runtime": 0.0026136,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.0032418,
                        "num_procs": 1
                    },
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 2
                    },
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 4
                    }
                ]
            }
        ],
        "response_time": 3.991224765777588,
        "total_tokens": 1518,
        "prompt_tokens": 1364
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_pixel_histogram",
        "parallelism_model": "serial",
        "prompt": "/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {",
        "outputs": [
            {
                "generated_output": " \nfor (const auto& pixel : image) {\n    bins[pixel]++;\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 2.1909329891204834,
        "total_tokens": 1342,
        "prompt_tokens": 1317
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "24_histogram_count_quartile",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use MPI to compute in parallel. Assume that MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nsize_t n = x.size();\nsize_t local_count[4] = {0, 0, 0, 0};\n\nfor (size_t i = rank; i < n; i += size) {\n    double value = x[i];\n    double fraction = value - static_cast<int>(value);\n    if (fraction >= 0.0 && fraction < 0.25) {\n        local_count[0]++;\n    } else if (fraction >= 0.25 && fraction < 0.5) {\n        local_count[1]++;\n    } else if (fraction >= 0.5 && fraction < 0.75) {\n        local_count[2]++;\n    } else if (fraction >= 0.75 && fraction < 1.0) {\n        local_count[3]++;\n    }\n}\n\nMPI_Reduce(local_count, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": true,
                "are_all_valid": true,
                "best_sequential_runtime": 0.093775,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.0867904,
                        "num_procs": 1
                    },
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.0412636,
                        "num_procs": 2
                    },
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.02378575,
                        "num_procs": 4
                    }
                ]
            }
        ],
        "response_time": 4.77669620513916,
        "total_tokens": 1719,
        "prompt_tokens": 1430
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "24_histogram_count_quartile",
        "parallelism_model": "serial",
        "prompt": "/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {",
        "outputs": [
            {
                "generated_output": " \n    for (const auto& value : x) {\n        double fractionalPart = value - static_cast<int>(value);\n        if (fractionalPart < 0.25) {\n            bins[0]++;\n        } else if (fractionalPart < 0.5) {\n            bins[1]++;\n        } else if (fractionalPart < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": true,
                "are_all_valid": true,
                "best_sequential_runtime": 0.0823834418,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.0848284332
                    }
                ]
            }
        ],
        "response_time": 5.939435005187988,
        "total_tokens": 1504,
        "prompt_tokens": 1382
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "22_histogram_count_quadrants",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use MPI to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of points. The result is stored in bins on rank 0.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {",
        "outputs": [
            {
                "generated_output": " \n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t localCount[4] = {0, 0, 0, 0};\n\n    for (const auto& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            localCount[0]++;\n        } else if (point.x < 0 && point.y > 0) {\n            localCount[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            localCount[2]++;\n        } else if (point.x > 0 && point.y < 0) {\n            localCount[3]++;\n        }\n    }\n\n    std::array<size_t, 4> globalCounts;\n    MPI_Reduce(localCount, globalCounts.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = globalCounts;\n    }\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": true,
                "are_all_valid": false,
                "best_sequential_runtime": 0.0578638,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.0573504,
                        "num_procs": 1
                    },
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 2
                    },
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 4
                    }
                ]
            }
        ],
        "response_time": 4.247654914855957,
        "total_tokens": 1673,
        "prompt_tokens": 1389
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "22_histogram_count_quadrants",
        "parallelism_model": "serial",
        "prompt": "struct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {",
        "outputs": [
            {
                "generated_output": " \n    std::fill(bins.begin(), bins.end(), 0);\n\n    #pragma omp parallel for reduction(+:bins)\n    for (const auto& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            bins[0]++;\n        } else if (point.x < 0 && point.y > 0) {\n            bins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            bins[2]++;\n        } else if (point.x > 0 && point.y < 0) {\n            bins[3]++;\n        }\n    }\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": true,
                "are_all_valid": true,
                "best_sequential_runtime": 0.0600405668,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.0586872334
                    }
                ]
            }
        ],
        "response_time": 3.1180362701416016,
        "total_tokens": 1508,
        "prompt_tokens": 1342
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "23_histogram_first_letter_counts",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of s. The result is stored in bins on rank 0.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nstd::array<size_t, 26> localBins{0};\n\nfor (const auto& str : s) {\n    if (!str.empty()) {\n        char firstChar = str[0];\n        if (firstChar >= 'a' && firstChar <= 'z') {\n            localBins[firstChar - 'a']++;\n        }\n    }\n}\n\nMPI_Reduce(&localBins[0], &bins[0], 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 3.377101182937622,
        "total_tokens": 1587,
        "prompt_tokens": 1411
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "23_histogram_first_letter_counts",
        "parallelism_model": "serial",
        "prompt": "/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {",
        "outputs": [
            {
                "generated_output": " \nfor (const auto& str : s) {\n    if (!str.empty()) {\n        bins[str[0] - 'a']++;\n    }\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 2.457379102706909,
        "total_tokens": 1407,
        "prompt_tokens": 1364
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "21_histogram_bin_0-100",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n// Each rank computes its local count\nstd::array<size_t, 10> localBins = {0};\nfor (const auto& value : x) {\n    size_t binIndex = static_cast<size_t>(value) / 10;\n    localBins[binIndex]++;\n}\n\n// Gather all local bins to rank 0\nstd::array<size_t, 10> globalBins;\nMPI_Reduce(&localBins[0], &globalBins[0], 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n// Rank 0 stores the result in bins\nif (rank == 0) {\n    bins = globalBins;\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 3.745824098587036,
        "total_tokens": 1613,
        "prompt_tokens": 1393
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "21_histogram_bin_0-100",
        "parallelism_model": "serial",
        "prompt": "/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {",
        "outputs": [
            {
                "generated_output": " \nfor (const auto& value : x) {\n    size_t index = static_cast<size_t>(value) / 10;\n    if (index < 10) {\n        bins[index]++;\n    }\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 2.893483877182007,
        "total_tokens": 1407,
        "prompt_tokens": 1346
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "39_search_xor_contains",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Use MPI to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Return the result on rank 0.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nbool hasValInX = false;\nbool hasValInY = false;\n\nfor (const auto& elem : x) {\n    if (elem == val) {\n        hasValInX = true;\n        break;\n    }\n}\n\nfor (const auto& elem : y) {\n    if (elem == val) {\n        hasValInY = true;\n        break;\n    }\n}\n\nbool globalResult = false;\nMPI_Reduce(&hasValInX, &globalResult, 1, MPI_C_BOOL, MPI_LAND, 0, MPI_COMM_WORLD);\nMPI_Reduce(&hasValInY, &globalResult, 1, MPI_C_BOOL, MPI_LOR, 0, MPI_COMM_WORLD);\n\nif (rank == 0) {\n    return globalResult;\n}\nreturn false;\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 5.033216953277588,
        "total_tokens": 1609,
        "prompt_tokens": 1362
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "39_search_xor_contains",
        "parallelism_model": "serial",
        "prompt": "/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {",
        "outputs": [
            {
                "generated_output": " \nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {\n    bool foundInX = std::find(x.begin(), x.end(), val) != x.end();\n    bool foundInY = std::find(y.begin(), y.end(), val) != y.end();\n    return foundInX != foundInY;\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 2.3165879249572754,
        "total_tokens": 1416,
        "prompt_tokens": 1316
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "37_search_find_the_closest_number_to_pi",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Use MPI to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(std::vector<double> const& x) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\ndouble localMinDiff = std::abs(M_PI - x[rank]);\ndouble globalMinDiff = localMinDiff;\nsize_t localClosestIndex = rank;\n\nfor (size_t i = rank + size; i < x.size(); i += size) {\n    double diff = std::abs(M_PI - x[i]);\n    if (diff < localMinDiff) {\n        localMinDiff = diff;\n        localClosestIndex = i;\n    }\n}\n\nMPI_Allreduce(&localMinDiff, &globalMinDiff, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\nMPI_Allreduce(&localClosestIndex, &globalClosestIndex, 1, MPI_UNSIGNED_LONG_LONG, MPI_MINLOC, MPI_COMM_WORLD);\n\nif (rank == 0) {\n    return globalClosestIndex;\n}\nreturn 0;\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 4.398663282394409,
        "total_tokens": 1582,
        "prompt_tokens": 1324
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "37_search_find_the_closest_number_to_pi",
        "parallelism_model": "serial",
        "prompt": "/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(std::vector<double> const& x) {",
        "outputs": [
            {
                "generated_output": " \nif (x.empty()) {\n    return std::numeric_limits<size_t>::max();\n}\n\nconst double pi = M_PI;\nsize_t closestIndex = 0;\ndouble minDifference = std::abs(x[0] - pi);\n\n#pragma omp parallel for reduction(min:minDifference)\nfor (size_t i = 1; i < x.size(); ++i) {\n    double difference = std::abs(x[i] - pi);\n    if (difference < minDifference) {\n        minDifference = difference;\n        closestIndex = i;\n    }\n}\n\nreturn closestIndex;\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": true,
                "are_all_valid": true,
                "best_sequential_runtime": 5e-08,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 5e-08
                    }
                ]
            }
        ],
        "response_time": 5.568962812423706,
        "total_tokens": 1435,
        "prompt_tokens": 1280
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "38_search_find_the_first_even_number",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the index of the first even number in the vector x.\n   Use MPI to parallelize the search. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(std::vector<int> const& x) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nsize_t local_first_even = x.size();\nfor (size_t i = rank; i < x.size(); i += size) {\n    if (x[i] % 2 == 0) {\n        local_first_even = i;\n        break;\n    }\n}\n\nsize_t global_first_even = x.size();\nMPI_Reduce(&local_first_even, &global_first_even, 1, MPI_SIZE_T, MPI_MIN, 0, MPI_COMM_WORLD);\n\nif (rank == 0) {\n    return global_first_even;\n}\nreturn x.size();\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 4.432875156402588,
        "total_tokens": 1534,
        "prompt_tokens": 1335
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "38_search_find_the_first_even_number",
        "parallelism_model": "serial",
        "prompt": "/* Return the index of the first even number in the vector x.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(std::vector<int> const& x) {",
        "outputs": [
            {
                "generated_output": " \nif (x.empty()) {\n    return x.size(); // Return size if the vector is empty\n}\n\n#pragma omp parallel for\nfor (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n        return i;\n    }\n}\n\nreturn x.size(); // Return size if no even number is found\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 2.770296812057495,
        "total_tokens": 1391,
        "prompt_tokens": 1290
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "35_search_search_for_last_struct_by_key",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct Book {\n   std::string title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Use MPI to search in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of books. Return the result on rank 0.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nsize_t localLastIndex = static_cast<size_t>(-1);\nfor (size_t i = rank; i < books.size(); i += size) {\n    if (books[i].pages < 100) {\n        localLastIndex = i;\n    }\n}\n\nsize_t globalLastIndex = static_cast<size_t>(-1);\nMPI_Reduce(&localLastIndex, &globalLastIndex, 1, MPI_SIZE_T, MPI_MAX, 0, MPI_COMM_WORLD);\n\nif (rank == 0) {\n    return globalLastIndex;\n}\nreturn 0;\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 3.2414541244506836,
        "total_tokens": 1547,
        "prompt_tokens": 1351
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "35_search_search_for_last_struct_by_key",
        "parallelism_model": "serial",
        "prompt": "struct Book {\n   std::string title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {",
        "outputs": [
            {
                "generated_output": " \nif (books.empty()) {\n    return books.size();\n}\n\nsize_t lastIndex = books.size();\nfor (size_t i = 0; i < books.size(); ++i) {\n    if (books[i].pages < 100) {\n        lastIndex = i;\n    }\n}\n\nreturn lastIndex;\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 2.2008650302886963,
        "total_tokens": 1397,
        "prompt_tokens": 1308
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "36_search_check_if_array_contains_value",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Use MPI to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(std::vector<int> const& x, int target) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nbool found = false;\n\nfor (const auto& value : x) {\n    if (value == target) {\n        found = true;\n        break;\n    }\n}\n\nMPI_Reduce(&found, &found, 1, MPI_C_BOOL, MPI_LOR, 0, MPI_COMM_WORLD);\n\nif (rank == 0) {\n    return found;\n}\nreturn false;\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 2.6425349712371826,
        "total_tokens": 1467,
        "prompt_tokens": 1323
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "36_search_check_if_array_contains_value",
        "parallelism_model": "serial",
        "prompt": "/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(std::vector<int> const& x, int target) {",
        "outputs": [
            {
                "generated_output": " \nfor (const auto& value : x) {\n    if (value == target) {\n        return true;\n    }\n}\nreturn false;\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 1.963698148727417,
        "total_tokens": 1318,
        "prompt_tokens": 1279
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "05_fft_inverse_fft",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* forward declare fft. computes fourier transform in-place */\nvoid fft(std::vector<std::complex<double>> &x);\n\n/* Compute the inverse fourier transform of x in-place.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\nvoid ifft(std::vector<std::complex<double>> &x) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nif (size == 1) {\n    fft(x);\n    for (auto &val : x) {\n        val = std::conj(val) / x.size();\n    }\n    return;\n}\n\n// Each process computes its own FFT\nfft(x);\n\n// Gather all results on rank 0\nstd::vector<std::complex<double>> allX(x.size() * size);\nMPI_Gather(&x[0], x.size(), MPI_CXX_COMPLEX, &allX[0], x.size(), MPI_CXX_COMPLEX, 0, MPI_COMM_WORLD);\n\nif (rank == 0) {\n    // Rank 0 computes the inverse FFT of all gathered data\n    for (auto &val : allX) {\n        val = std::conj(val) / allX.size();\n    }\n    fft(allX);\n\n    // Scatter the result back to the processes\n    std::vector<std::complex<double>> localResult(x.size());\n    MPI_Scatter(&allX[0], x.size(), MPI_CXX_COMPLEX, &localResult[0], x.size(), MPI_CXX_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Copy the result to x\n    x = localResult;\n} else {\n    // Other ranks do nothing, as they only needed to gather data\n    MPI_Scatter(nullptr, 0, MPI_CXX_COMPLEX, nullptr, 0, MPI_CXX_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 5.875576019287109,
        "total_tokens": 1840,
        "prompt_tokens": 1429
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "05_fft_inverse_fft",
        "parallelism_model": "serial",
        "prompt": "/* forward declare fft. computes fourier transform in-place */\nvoid fft(std::vector<std::complex<double>> &x);\n\n/* Compute the inverse fourier transform of x in-place.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\nvoid ifft(std::vector<std::complex<double>> &x) {",
        "outputs": [
            {
                "generated_output": " \nvoid ifft(std::vector<std::complex<double>> &x) {\n    int n = x.size();\n    fft(x);  // FFT to get the conjugate of the ifft result\n    for (auto &val : x) {\n        val = std::conj(val);  // Conjugate the result to get the ifft\n    }\n    for (auto &val : x) {\n        val /= n;  // Normalize the result by the number of elements\n    }\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 3.71111798286438,
        "total_tokens": 1504,
        "prompt_tokens": 1383
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "08_fft_split_fft",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i) {",
        "outputs": [
            {
                "generated_output": " \n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int n_local = n / size;\n\n    std::vector<std::complex<double>> x_local(n_local);\n    MPI_Scatter(x.data(), n_local, MPI_CXX_DOUBLE_COMPLEX, x_local.data(), n_local, MPI_CXX_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT on local data\n    // This is a placeholder for the actual FFT implementation\n    std::vector<std::complex<double>> fft_local = fft_serial(x_local);\n\n    // Gather results back to rank 0\n    if (rank == 0) {\n        r.resize(n);\n        i.resize(n);\n    }\n    MPI_Gather(fft_local.data(), n_local, MPI_CXX_DOUBLE_COMPLEX, r.data(), n_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(fft_local.data(), n_local, MPI_CXX_DOUBLE_COMPLEX, i.data(), n_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 4.923035144805908,
        "total_tokens": 1737,
        "prompt_tokens": 1411
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "08_fft_split_fft",
        "parallelism_model": "serial",
        "prompt": "/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i) {",
        "outputs": [
            {
                "generated_output": " \nconst int N = x.size();\nr.resize(N);\ni.resize(N);\n\nfor (int k = 0; k < N; ++k) {\n    std::complex<double> sum_real = 0.0;\n    std::complex<double> sum_imag = 0.0;\n    for (int n = 0; n < N; ++n) {\n        double theta = -2.0 * M_PI * k * n / N;\n        std::complex<double> exp_theta(cos(theta), sin(theta));\n        sum_real += x[n] * exp_theta;\n        sum_imag += x[n] * std::conj(exp_theta);\n    }\n    r[k] = sum_real.real();\n    i[k] = sum_imag.imag();\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 3.847356081008911,
        "total_tokens": 1568,
        "prompt_tokens": 1365
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "07_fft_fft_conjugate",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fftConjugate(std::vector<std::complex<double>> &x) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nconst int n = x.size();\nconst double pi = 3.14159265358979323846;\n\nstd::vector<std::complex<double>> localX(n / size);\nstd::copy(x.begin() + rank * (n / size), x.begin() + (rank + 1) * (n / size), localX.begin());\n\nfor (int k = 0; k < n / size; ++k) {\n    std::complex<double> sum(0.0, 0.0);\n    for (int j = 0; j < n; ++j) {\n        double theta = -2.0 * pi * j * k / n;\n        std::complex<double> expVal(cos(theta), sin(theta));\n        sum += localX[k] * expVal;\n    }\n    localX[k] = sum;\n}\n\nstd::vector<std::complex<double>> globalX(n);\nMPI_Gather(localX.data(), n / size, MPI_CXX_DOUBLE_COMPLEX, globalX.data(), n / size, MPI_CXX_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\nif (rank == 0) {\n    for (int i = 0; i < n; ++i) {\n        globalX[i] = std::conj(globalX[i]);\n    }\n}\n\nstd::copy(globalX.begin(), globalX.end(), x.begin());\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 1
                    },
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 2
                    },
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 4
                    }
                ]
            }
        ],
        "response_time": 5.185296058654785,
        "total_tokens": 1786,
        "prompt_tokens": 1382
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "07_fft_fft_conjugate",
        "parallelism_model": "serial",
        "prompt": "/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fftConjugate(std::vector<std::complex<double>> &x) {",
        "outputs": [
            {
                "generated_output": " \n#include <complex>\n#include <vector>\n#include <cmath>\n\nvoid fftConjugate(std::vector<std::complex<double>> &x) {\n    int n = x.size();\n    if (n <= 1) return;\n\n    std::vector<std::complex<double>> even = std::vector<std::complex<double>>(n / 2);\n    std::vector<std::complex<double>> odd = std::vector<std::complex<double>>(n / 2);\n\n    for (int i = 0; i < n / 2; ++i) {\n        even[i] = x[2 * i];\n        odd[i] = x[2 * i + 1];\n    }\n\n    fftConjugate(even);\n    fftConjugate(odd);\n\n    for (int k = 0; k < n / 2; ++k) {\n        std::complex<double> t = std::polar(1.0, -2 * M_PI * k / n) * odd[k];\n        x[k] = even[k] + t;\n        x[k + n / 2] = even[k] - t;\n    }\n\n    for (int i = 0; i < n / 2; ++i) {\n        x[i].imag(-x[i].imag());\n        x[i + n / 2].imag(-x[i + n / 2].imag());\n    }\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 4.887662172317505,
        "total_tokens": 1696,
        "prompt_tokens": 1336
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "09_fft_fft_out_of_place",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute the fourier transform of x. Store the result in output.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<std::complex<double>> &output) {",
        "outputs": [
            {
                "generated_output": " \n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    std::vector<std::complex<double>> local_x(local_n);\n    MPI_Scatter(x.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform local FFT on local_x\n    std::vector<std::complex<double>> local_output(local_n);\n    for (int k = 0; k < local_n; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int j = 0; j < local_n; ++j) {\n            double theta = -2.0 * M_PI * j * k / n;\n            std::complex<double> w(cos(theta), sin(theta));\n            sum += local_x[j] * w;\n        }\n        local_output[k] = sum;\n    }\n\n    MPI_Gather(local_output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": [
                    {
                        "did_run": false,
                        "is_valid": null,
                        "runtime": null,
                        "num_procs": 1
                    },
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 2
                    },
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 4
                    }
                ]
            }
        ],
        "response_time": 5.352123022079468,
        "total_tokens": 1722,
        "prompt_tokens": 1366
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "09_fft_fft_out_of_place",
        "parallelism_model": "serial",
        "prompt": "/* Compute the fourier transform of x. Store the result in output.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<std::complex<double>> &output) {",
        "outputs": [
            {
                "generated_output": " \n#include <complex>\n#include <vector>\n#include <cmath>\n\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<std::complex<double>> &output) {\n    int N = x.size();\n    output.resize(N);\n\n    if (N <= 1) {\n        output = x;\n        return;\n    }\n\n    std::vector<std::complex<double>> even(N / 2);\n    std::vector<std::complex<double>> odd(N / 2);\n\n    for (int i = 0; i < N / 2; ++i) {\n        even[i] = x[2 * i];\n        odd[i] = x[2 * i + 1];\n    }\n\n    fft(even, output);\n    fft(odd, output);\n\n    for (int k = 0; k < N / 2; ++k) {\n        std::complex<double> t = std::polar(1.0, -2 * M_PI * k / N) * odd[k];\n        output[k] = output[k] + t;\n        output[k + N / 2] = output[k + N / 2] - t;\n    }\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 4.866344928741455,
        "total_tokens": 1652,
        "prompt_tokens": 1342
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "06_fft_dft",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute the discrete fourier transform of x. Store the result in output.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\nvoid dft(std::vector<double> const& x, std::vector<std::complex<double>> &output) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nint n = x.size();\nstd::vector<std::complex<double>> localOutput(n / size);\n\nfor (int k = rank; k < n; k += size) {\n    std::complex<double> sum(0.0, 0.0);\n    for (int t = 0; t < n; ++t) {\n        double angle = 2.0 * M_PI * t * k / n;\n        std::complex<double> expVal(cos(angle), sin(angle));\n        sum += x[t] * expVal;\n    }\n    localOutput[k / size] = sum;\n}\n\nMPI_Gather(&localOutput[0], n / size, MPI_CXX_DOUBLE_COMPLEX, &output[0], n / size, MPI_CXX_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\nif (rank == 0) {\n    // The root process gathers all the results from other processes\n    // No need to do anything here as the output vector is already populated\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 4.77333402633667,
        "total_tokens": 1586,
        "prompt_tokens": 1298
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "06_fft_dft",
        "parallelism_model": "serial",
        "prompt": "/* Compute the discrete fourier transform of x. Store the result in output.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\nvoid dft(std::vector<double> const& x, std::vector<std::complex<double>> &output) {",
        "outputs": [
            {
                "generated_output": " \n    int N = x.size();\n    output.resize(N);\n\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            std::complex<double> exp_val(cos(angle), sin(angle));\n            sum += x[n] * exp_val;\n        }\n        output[k] = sum;\n    }\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null
                    }
                ]
            }
        ],
        "response_time": 3.8180317878723145,
        "total_tokens": 1420,
        "prompt_tokens": 1274
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "33_scan_reverse_prefix_sum",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute the reverse prefix sum of the vector x into output.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Store the result in output on rank 0.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\nvoid reversePrefixSum(std::vector<int> const& x, std::vector<int> &output) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nint n = x.size();\nstd::vector<int> localSum(n, 0);\nstd::vector<int> globalSum(n, 0);\n\n// Compute local sum\nfor (int i = rank; i < n; i += size) {\n    localSum[i] = (i > 0 ? localSum[i - 1] : 0) + x[i];\n}\n\n// Reduce local sums to global sums\nMPI_Reduce(&localSum[0], &globalSum[0], n, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n// Reverse the global sum to get the output\nif (rank == 0) {\n    for (int i = 0; i < n; ++i) {\n        output[i] = (i > 0 ? globalSum[i - 1] : 0) + globalSum[i];\n    }\n}\n\n// Broadcast the result to all ranks\nMPI_Bcast(&output[0], n, MPI_INT, 0, MPI_COMM_WORLD);\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 4.617210865020752,
        "total_tokens": 1648,
        "prompt_tokens": 1354
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "33_scan_reverse_prefix_sum",
        "parallelism_model": "serial",
        "prompt": "/* Compute the reverse prefix sum of the vector x into output.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\nvoid reversePrefixSum(std::vector<int> const& x, std::vector<int> &output) {",
        "outputs": [
            {
                "generated_output": " \nif (x.empty()) {\n    return;\n}\n\noutput.resize(x.size());\noutput[x.size() - 1] = x[x.size() - 1];\n\nfor (int i = x.size() - 2; i >= 0; --i) {\n    output[i] = output[i + 1] + x[i];\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 2.969849109649658,
        "total_tokens": 1404,
        "prompt_tokens": 1308
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "30_scan_prefix_sum",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute the prefix sum of the vector x into output.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Store the result in output on rank 0.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n// Each process has a local prefix sum\nstd::vector<double> localPrefixSum(x.size());\ndouble localSum = 0.0;\nfor (size_t i = 0; i < x.size(); ++i) {\n    localSum += x[i];\n    localPrefixSum[i] = localSum;\n}\n\n// Gather all local prefix sums to rank 0\nstd::vector<double> allLocalPrefixSums(size * x.size());\nMPI_Gather(localPrefixSum.data(), x.size(), MPI_DOUBLE, allLocalPrefixSums.data(), x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\nif (rank == 0) {\n    // Compute the global prefix sum on rank 0\n    for (int i = 0; i < size; ++i) {\n        for (size_t j = i * x.size(); j < (i + 1) * x.size(); ++j) {\n            output[j % x.size()] = allLocalPrefixSums[j];\n        }\n    }\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 6.490025997161865,
        "total_tokens": 1610,
        "prompt_tokens": 1311
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "30_scan_prefix_sum",
        "parallelism_model": "serial",
        "prompt": "/* Compute the prefix sum of the vector x into output.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {",
        "outputs": [
            {
                "generated_output": " \nif (x.size() != output.size()) {\n    throw std::invalid_argument(\"Input and output vectors must be of the same size\");\n}\n\n#pragma omp parallel for\nfor (size_t i = 1; i < x.size(); ++i) {\n    output[i] = x[i] + output[i - 1];\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 3.460361957550049,
        "total_tokens": 1356,
        "prompt_tokens": 1266
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "34_scan_largest_contiguous_subarray_sum",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\nint maximumSubarray(std::vector<int> const& x) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nint local_max = 0;\nint global_max = 0;\nint n = x.size();\n\n// Each process finds the maximum subarray sum in its portion of the array\nfor (int i = rank; i < n; i += size) {\n    int current_sum = 0;\n    for (int j = i; j < n; ++j) {\n        current_sum += x[j];\n        if (current_sum > local_max) {\n            local_max = current_sum;\n        }\n    }\n}\n\n// Reduce the local maximums to the global maximum\nMPI_Reduce(&local_max, &global_max, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\nif (rank == 0) {\n    return global_max;\n}\nreturn 0;\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 9.192867994308472,
        "total_tokens": 1610,
        "prompt_tokens": 1365
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "34_scan_largest_contiguous_subarray_sum",
        "parallelism_model": "serial",
        "prompt": "/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\nint maximumSubarray(std::vector<int> const& x) {",
        "outputs": [
            {
                "generated_output": " \nif (x.empty()) {\n    return 0;\n}\n\nint maxSum = x[0];\nint currentSum = x[0];\n\nfor (size_t i = 1; i < x.size(); ++i) {\n    currentSum = std::max(x[i], currentSum + x[i]);\n    maxSum = std::max(maxSum, currentSum);\n}\n\nreturn maxSum;\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 3.0711002349853516,
        "total_tokens": 1426,
        "prompt_tokens": 1321
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "32_scan_sum_of_prefix_sum_array",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nint n = x.size();\nstd::vector<double> localPrefixSum(n, 0.0);\nstd::vector<double> globalPrefixSum(n, 0.0);\n\n// Compute local prefix sum\nfor (int i = 0; i < n; ++i) {\n    localPrefixSum[i] = (i > 0 ? localPrefixSum[i - 1] : 0) + x[i];\n}\n\n// Gather local prefix sums to all ranks\nMPI_Allgather(localPrefixSum.data(), n, MPI_DOUBLE, globalPrefixSum.data(), n, MPI_DOUBLE, MPI_COMM_WORLD);\n\n// Compute global prefix sum on rank 0\nif (rank == 0) {\n    for (int i = 1; i < size; ++i) {\n        for (int j = 0; j < n; ++j) {\n            globalPrefixSum[j] += globalPrefixSum[j * size + i];\n        }\n    }\n}\n\n// Broadcast the global prefix sum to all ranks\nMPI_Bcast(globalPrefixSum.data(), n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n// Sum the global prefix sum on rank 0\ndouble sum = 0.0;\nif (rank == 0) {\n    for (int i = 0; i < n; ++i) {\n        sum += globalPrefixSum[i];\n    }\n}\n\nreturn sum;\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": false,
                "are_any_valid": true,
                "are_all_valid": false,
                "best_sequential_runtime": 0.0164514,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.0223408,
                        "num_procs": 1
                    },
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 2
                    },
                    {
                        "did_run": false,
                        "is_valid": null,
                        "runtime": null,
                        "num_procs": 4
                    }
                ]
            }
        ],
        "response_time": 6.757018089294434,
        "total_tokens": 1673,
        "prompt_tokens": 1285
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "32_scan_sum_of_prefix_sum_array",
        "parallelism_model": "serial",
        "prompt": "/* Compute the prefix sum array of the vector x and return its sum.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {",
        "outputs": [
            {
                "generated_output": " \nif (x.empty()) {\n    return 0.0;\n}\n\nstd::vector<double> prefixSum(x.size());\nprefixSum[0] = x[0];\n\n#pragma omp parallel for\nfor (size_t i = 1; i < x.size(); ++i) {\n    prefixSum[i] = prefixSum[i - 1] + x[i];\n}\n\ndouble totalSum = 0.0;\n#pragma omp parallel for reduction(+:totalSum)\nfor (size_t i = 0; i < x.size(); ++i) {\n    totalSum += prefixSum[i];\n}\n\nreturn totalSum;\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": true,
                "are_all_valid": true,
                "best_sequential_runtime": 0.0188414916,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.0216111248
                    }
                ]
            }
        ],
        "response_time": 4.298583030700684,
        "total_tokens": 1408,
        "prompt_tokens": 1242
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "31_scan_scan_with_min_function",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nstd::vector<float> localMin(x.size());\nstd::vector<float> globalMin(x.size());\n\n// Each rank computes its local minimums\nfor (size_t i = rank; i < x.size(); i += size) {\n    localMin[i] = x[i];\n    for (size_t j = 0; j < i; j++) {\n        if (x[j] < localMin[i]) {\n            localMin[i] = x[j];\n        }\n    }\n}\n\n// Gather all local minimums to rank 0\nMPI_Allgather(&localMin[rank], x.size() / size, MPI_FLOAT, &globalMin[0], x.size() / size, MPI_FLOAT, MPI_COMM_WORLD);\n\n// Update the original vector x on rank 0\nif (rank == 0) {\n    for (size_t i = 0; i < x.size(); i++) {\n        x[i] = globalMin[i];\n    }\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 5.137488842010498,
        "total_tokens": 1659,
        "prompt_tokens": 1371
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "31_scan_scan_with_min_function",
        "parallelism_model": "serial",
        "prompt": "/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {",
        "outputs": [
            {
                "generated_output": " \nif (x.empty()) {\n    return;\n}\n\nfloat minVal = x[0];\nfor (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] < minVal) {\n        minVal = x[i];\n    }\n    x[i] = minVal;\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 2.0817198753356934,
        "total_tokens": 1412,
        "prompt_tokens": 1325
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "17_graph_highest_degree",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nint localMaxDegree = 0;\nfor (int i = rank; i < N; i += size) {\n    int degree = 0;\n    for (int j = 0; j < N; ++j) {\n        if (A[i * N + j] == 1) {\n            degree++;\n        }\n    }\n    if (degree > localMaxDegree) {\n        localMaxDegree = degree;\n    }\n}\n\nint globalMaxDegree;\nMPI_Reduce(&localMaxDegree, &globalMaxDegree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\nif (rank == 0) {\n    return globalMaxDegree;\n}\nreturn 0;\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 3.32662296295166,
        "total_tokens": 1562,
        "prompt_tokens": 1347
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "17_graph_highest_degree",
        "parallelism_model": "serial",
        "prompt": "/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {",
        "outputs": [
            {
                "generated_output": " \nint maxDegree(std::vector<int> const& A, size_t N) {\n    int max_degree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                ++degree;\n            }\n        }\n        if (degree > max_degree) {\n            max_degree = degree;\n        }\n    }\n    return max_degree;\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 2.6637561321258545,
        "total_tokens": 1448,
        "prompt_tokens": 1302
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "16_graph_largest_component",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(std::vector<int> const& A, size_t N) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n// Each rank has a complete copy of A, so we need to determine the size of the local submatrix\nsize_t localN = N / size;\nif (rank < N % size) {\n    localN++;\n}\n\n// Create local adjacency matrix for each rank\nstd::vector<int> localA(localN * N);\nfor (size_t i = 0; i < localN; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        localA[i * N + j] = A[(rank * localN + i) * N + j];\n    }\n}\n\n// Find the size of the largest component in the local submatrix\nstd::vector<int> componentSizes(localN, 0);\nstd::vector<bool> visited(localN, false);\nstd::function<void(int)> dfs = [&](int node) {\n    visited[node] = true;\n    componentSizes[node] = 1;\n    for (int neighbor = 0; neighbor < localN; ++neighbor) {\n        if (localA[node * N + neighbor] == 1 && !visited[neighbor]) {\n            dfs(neighbor);\n            componentSizes[node] += componentSizes[neighbor];\n        }\n    }\n};\n\nfor (size_t i = 0; i < localN; ++i) {\n    if (!visited[i]) {\n        dfs(i);\n    }\n}\n\n// Gather component sizes from all ranks\nstd::vector<int> allComponentSizes(size * localN, 0);\nMPI_Gather(componentSizes.data(), localN, MPI_INT, allComponentSizes.data(), localN, MPI_INT, 0, MPI_COMM_WORLD);\n\nif (rank == 0) {\n    // Find the largest component size across all ranks\n    int maxComponentSize = 0;\n    for (size_t i = 0; i < size * localN; ++i) {\n        if (allComponentSizes[i] > maxComponentSize) {\n            maxComponentSize = allComponentSizes[i];\n        }\n    }\n    return maxComponentSize;\n}\nreturn 0;\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": true,
                "are_all_valid": false,
                "best_sequential_runtime": 0.1094032,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.2000626,
                        "num_procs": 1
                    },
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 2
                    },
                    {
                        "did_run": true,
                        "is_valid": false,
                        "runtime": null,
                        "num_procs": 4
                    }
                ]
            }
        ],
        "response_time": 7.06224799156189,
        "total_tokens": 1907,
        "prompt_tokens": 1347
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "16_graph_largest_component",
        "parallelism_model": "serial",
        "prompt": "/* Return the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(std::vector<int> const& A, size_t N) {",
        "outputs": [
            {
                "generated_output": " \n#include <vector>\n#include <algorithm>\n\nclass UnionFind {\npublic:\n    UnionFind(size_t size) : parent(size), rank(size, 0) {\n        for (size_t i = 0; i < size; ++i) {\n            parent[i] = i;\n        }\n    }\n\n    size_t find(size_t p) {\n        if (parent[p] != p) {\n            parent[p] = find(parent[p]); // Path compression\n        }\n        return parent[p];\n    }\n\n    void unite(size_t p, size_t q) {\n        size_t rootP = find(p);\n        size_t rootQ = find(q);\n        if (rootP != rootQ) {\n            if (rank[rootP] > rank[rootQ]) {\n                parent[rootQ] = rootP;\n            } else if (rank[rootP] < rank[rootQ]) {\n                parent[rootP] = rootQ;\n            } else {\n                parent[rootQ] = rootP;\n                rank[rootP]++;\n            }\n        }\n    }\n\nprivate:\n    std::vector<size_t> parent;\n    std::vector<size_t> rank;\n};\n\nint largestComponent(std::vector<int> const& A, size_t N) {\n    UnionFind uf(N);\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = i + 1; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                uf.unite(i, j);\n            }\n        }\n    }\n\n    std::vector<size_t> componentSizes(N, 0);\n    for (size_t i = 0; i < N; ++i) {\n        componentSizes[uf.find(i)]++;\n    }\n\n    size_t maxComponentSize = *std::max_element(componentSizes.begin(), componentSizes.end());\n    return static_cast<int>(maxComponentSize);\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 6.4957520961761475,
        "total_tokens": 1819,
        "prompt_tokens": 1302
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "19_graph_shortest_path",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The shortest path length is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {",
        "outputs": [
            {
                "generated_output": " \n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Each rank has a copy of the adjacency matrix A\n    std::vector<int> localA(A.begin() + rank * N * N / size, A.begin() + (rank + 1) * N * N / size);\n\n    // Initialize distances with infinity\n    std::vector<int> localDist(N, INT_MAX);\n    localDist[source] = 0;\n\n    // Perform a parallel BFS\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            if (localA[j * N + i] == 1) {\n                for (int k = 0; k < N; ++k) {\n                    if (localA[i * N + k] == 1) {\n                        if (localDist[i] + 1 < localDist[k]) {\n                            localDist[k] = localDist[i] + 1;\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    // Gather distances on rank 0\n    std::vector<int> globalDist(N, INT_MAX);\n    MPI_Reduce(&localDist[0], &globalDist[0], N, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalDist[dest];\n    }\n    return 0;\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 5.224894762039185,
        "total_tokens": 1752,
        "prompt_tokens": 1365
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "19_graph_shortest_path",
        "parallelism_model": "serial",
        "prompt": "/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {",
        "outputs": [
            {
                "generated_output": " \n#include <vector>\n#include <queue>\n#include <algorithm>\n\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {\n    if (source == dest) {\n        return 0;\n    }\n\n    std::vector<bool> visited(N, false);\n    std::queue<std::pair<int, int>> q; // pair of (current node, distance)\n\n    q.push({source, 0});\n    visited[source] = true;\n\n    while (!q.empty()) {\n        auto [current, distance] = q.front();\n        q.pop();\n\n        for (int neighbor = 0; neighbor < N; ++neighbor) {\n            if (A[current * N + neighbor] == 1 && !visited[neighbor]) {\n                if (neighbor == dest) {\n                    return distance + 1;\n                }\n                q.push({neighbor, distance + 1});\n                visited[neighbor] = true;\n            }\n        }\n    }\n\n    return -1; // If no path is found\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 4.485267162322998,
        "total_tokens": 1585,
        "prompt_tokens": 1318
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "15_graph_edge_count",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A represents a directed graph.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 6\n*/\nint edgeCount(std::vector<int> const& A, size_t N) {",
        "outputs": [
            {
                "generated_output": " \nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nint localCount = 0;\nsize_t localN = N / size;\n\nfor (size_t i = rank * localN; i < (rank + 1) * localN; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        if (A[i * N + j] == 1) {\n            localCount++;\n        }\n    }\n}\n\nint globalCount = 0;\nMPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\nif (rank == 0) {\n    return globalCount;\n}\nreturn 0;\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": true,
                "are_any_valid": true,
                "are_all_valid": true,
                "best_sequential_runtime": 0.0,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.1184762,
                        "num_procs": 1
                    },
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.0691578,
                        "num_procs": 2
                    },
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.0421539,
                        "num_procs": 4
                    }
                ]
            }
        ],
        "response_time": 3.8599841594696045,
        "total_tokens": 1555,
        "prompt_tokens": 1343
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "15_graph_edge_count",
        "parallelism_model": "serial",
        "prompt": "/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A represents a directed graph.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 6\n*/\nint edgeCount(std::vector<int> const& A, size_t N) {",
        "outputs": [
            {
                "generated_output": " \nint edgeCount(std::vector<int> const& A, size_t N) {\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] != 0) {\n                ++count;\n            }\n        }\n    }\n    return count;\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 2.292372941970825,
        "total_tokens": 1410,
        "prompt_tokens": 1298
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "18_graph_count_components",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n\t Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The component count is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(std::vector<int> const& A, size_t N) {",
        "outputs": [
            {
                "generated_output": " \n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Each rank has a complete copy of A, so we need to determine the local portion of A\n    size_t local_N = N / size;\n    if (rank < N % size) {\n        local_N++;\n    }\n    std::vector<int> local_A(local_N * N);\n\n    // Scatter the rows of A to all ranks\n    MPI_Scatter(A.data(), local_N * N, MPI_INT, local_A.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // Initialize component count and visited array\n    std::vector<bool> visited(local_N, false);\n    int local_component_count = 0;\n\n    // Function to perform DFS and mark visited nodes\n    std::function<void(int)> dfs = [&](int node) {\n        visited[node] = true;\n        for (int neighbor = 0; neighbor < N; ++neighbor) {\n            if (local_A[node * N + neighbor] == 1 && !visited[neighbor]) {\n                dfs(neighbor);\n            }\n        }\n    };\n\n    // Perform DFS on each unvisited node in the local portion of A\n    for (size_t i = 0; i < local_N; ++i) {\n        if (!visited[i]) {\n            dfs(i);\n            local_component_count++;\n        }\n    }\n\n    // Reduce the component counts to get the global count\n    int global_component_count;\n    MPI_Reduce(&local_component_count, &global_component_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_component_count;\n    }\n    return 0;\n}\n",
                "source_write_success": true,
                "did_build": true,
                "is_source_valid": true,
                "did_any_run": true,
                "did_all_run": false,
                "are_any_valid": true,
                "are_all_valid": false,
                "best_sequential_runtime": 0.1642932,
                "runs": [
                    {
                        "did_run": true,
                        "is_valid": true,
                        "runtime": 0.1571338,
                        "num_procs": 1
                    },
                    {
                        "did_run": false,
                        "is_valid": null,
                        "runtime": null,
                        "num_procs": 2
                    },
                    {
                        "did_run": false,
                        "is_valid": null,
                        "runtime": null,
                        "num_procs": 4
                    }
                ]
            }
        ],
        "response_time": 7.2741007804870605,
        "total_tokens": 1825,
        "prompt_tokens": 1345
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "18_graph_count_components",
        "parallelism_model": "serial",
        "prompt": "/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(std::vector<int> const& A, size_t N) {",
        "outputs": [
            {
                "generated_output": " \n#include <vector>\n#include <queue>\n\nvoid bfs(const std::vector<int>& A, std::vector<bool>& visited, int start) {\n    std::queue<int> q;\n    q.push(start);\n    visited[start] = true;\n    while (!q.empty()) {\n        int node = q.front();\n        q.pop();\n        for (int neighbor = 0; neighbor < A.size(); ++neighbor) {\n            if (A[node * A.size() + neighbor] == 1 && !visited[neighbor]) {\n                visited[neighbor] = true;\n                q.push(neighbor);\n            }\n        }\n    }\n}\n\nint componentCount(std::vector<int> const& A, size_t N) {\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (int i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            bfs(A, visited, i);\n            ++count;\n        }\n    }\n\n    return count;\n}\n",
                "source_write_success": true,
                "did_build": false,
                "is_source_valid": true,
                "did_any_run": false,
                "did_all_run": false,
                "are_any_valid": false,
                "are_all_valid": false,
                "best_sequential_runtime": null,
                "runs": null
            }
        ],
        "response_time": 4.54427695274353,
        "total_tokens": 1569,
        "prompt_tokens": 1299
    }
]